<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">

    

    <title>
      FactorAnalysis | Teaching ML 
    </title>

    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
    
      <meta name="author" content="transwarpio">
    
    

    <meta name="description" content="1. Introduction An extension of principal component analysis(PCA) in the sense of approximating covariance matrix. Goal To describe the covariance relationships among many variables in terms of a few">
<meta property="og:type" content="article">
<meta property="og:title" content="FactorAnalysis | Teaching ML">
<meta property="og:url" content="http://transwarpio.github.io/teaching_ml/2016/12/01/FactorAnalysis/index.html">
<meta property="og:site_name" content="Teaching ML">
<meta property="og:description" content="1. Introduction An extension of principal component analysis(PCA) in the sense of approximating covariance matrix. Goal To describe the covariance relationships among many variables in terms of a few">
<meta property="og:image" content="http://images.cnblogs.com/cnblogs_com/jerrylead/201105/201105111557474219.png">
<meta property="og:image" content="http://images.cnblogs.com/cnblogs_com/jerrylead/201105/201105111557493007.png">
<meta property="og:image" content="http://images.cnblogs.com/cnblogs_com/jerrylead/201105/20110511155750367.png">
<meta property="og:image" content="http://images.cnblogs.com/cnblogs_com/jerrylead/201105/201105111557566675.png">
<meta property="og:image" content="http://images.cnblogs.com/cnblogs_com/jerrylead/201105/201105111558042959.png">
<meta property="og:image" content="http://images.cnblogs.com/cnblogs_com/jerrylead/201105/201105111558444306.png">
<meta property="og:image" content="http://images.cnblogs.com/cnblogs_com/jerrylead/201105/201105111558474881.png">
<meta property="og:image" content="http://images.cnblogs.com/cnblogs_com/jerrylead/201105/201105111558484749.jpg">
<meta property="og:updated_time" content="2017-08-09T08:31:23.097Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="FactorAnalysis | Teaching ML">
<meta name="twitter:description" content="1. Introduction An extension of principal component analysis(PCA) in the sense of approximating covariance matrix. Goal To describe the covariance relationships among many variables in terms of a few">
<meta name="twitter:image" content="http://images.cnblogs.com/cnblogs_com/jerrylead/201105/201105111557474219.png">
    
    
    
      <link rel="icon" type="image/x-icon" href="/favicon.png">
    
    <link rel="stylesheet" href="/teaching_ml/css/uno.css">
    <link rel="stylesheet" href="/teaching_ml/css/highlight.css">
    <link rel="stylesheet" href="/teaching_ml/css/archive.css">
    <link rel="stylesheet" href="/teaching_ml/css/china-social-icon.css">

</head>
<body>

    <span class="mobile btn-mobile-menu">
        <i class="icon icon-list btn-mobile-menu__icon"></i>
        <i class="icon icon-x-circle btn-mobile-close__icon hidden"></i>
    </span>

    

<header class="panel-cover panel-cover--collapsed">


  <div class="panel-main">

  
    <div class="panel-main__inner panel-inverted">
    <div class="panel-main__content">

        

        <h1 class="panel-cover__title panel-title"><a href="/" title="link to homepage">Teaching ML</a></h1>
        <hr class="panel-cover__divider" />

        

        <div class="navigation-wrapper">

          <nav class="cover-navigation cover-navigation--primary">
            <ul class="navigation">

              
                
                <li class="navigation__item"><a href="/#blog" title="" class="blog-button">首页</a></li>
              

            </ul>
          </nav>

          <!-- ----------------------------
To add a new social icon simply duplicate one of the list items from below
and change the class in the <i> tag to match the desired social network
and then add your link to the <a>. Here is a full list of social network
classes that you can use:

    icon-social-500px
    icon-social-behance
    icon-social-delicious
    icon-social-designer-news
    icon-social-deviant-art
    icon-social-digg
    icon-social-dribbble
    icon-social-facebook
    icon-social-flickr
    icon-social-forrst
    icon-social-foursquare
    icon-social-github
    icon-social-google-plus
    icon-social-hi5
    icon-social-instagram
    icon-social-lastfm
    icon-social-linkedin
    icon-social-medium
    icon-social-myspace
    icon-social-path
    icon-social-pinterest
    icon-social-rdio
    icon-social-reddit
    icon-social-skype
    icon-social-spotify
    icon-social-stack-overflow
    icon-social-steam
    icon-social-stumbleupon
    icon-social-treehouse
    icon-social-tumblr
    icon-social-twitter
    icon-social-vimeo
    icon-social-xbox
    icon-social-yelp
    icon-social-youtube
    icon-social-zerply
    icon-mail

-------------------------------->

<!-- add social info here -->



        </div>

      </div>

    </div>

    <div class="panel-cover--overlay"></div>
  </div>
</header>

    <div class="content-wrapper">
        <div class="content-wrapper__inner entry">
            

<article class="post-container post-container--single">

  <header class="post-header">
    
    <h1 class="post-title">FactorAnalysis</h1>

    

    <div class="post-meta">
      <time datetime="2016-12-01" class="post-meta__date date">2016-12-01</time> 

      <span class="post-meta__tags tags">

          

          

      </span>
    </div>
    
    

  </header>

  <section id="post-content" class="article-content post">
    <h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p> An extension of <strong>principal component analysis(PCA)</strong> in the sense of approximating covariance matrix.</p>
<h3 id="Goal"><a href="#Goal" class="headerlink" title="Goal"></a>Goal</h3><ul>
<li>To describe the covariance relationships among many variables in terms of a few underlying unobservable random variables, called factors.</li>
<li>To reduce dimensions and solve the problem with n&lt;p.</li>
</ul>
<h2 id="2-Orthogonal-Factor-Model&#xFF08;&#x6B63;&#x4EA4;&#x56E0;&#x5B50;&#x6A21;&#x578B;&#xFF09;"><a href="#2-Orthogonal-Factor-Model&#xFF08;&#x6B63;&#x4EA4;&#x56E0;&#x5B50;&#x6A21;&#x578B;&#xFF09;" class="headerlink" title="2. Orthogonal Factor Model&#xFF08;&#x6B63;&#x4EA4;&#x56E0;&#x5B50;&#x6A21;&#x578B;&#xFF09;"></a>2. Orthogonal Factor Model&#xFF08;&#x6B63;&#x4EA4;&#x56E0;&#x5B50;&#x6A21;&#x578B;&#xFF09;</h2><h3 id="A-Factor-Analysis-Example"><a href="#A-Factor-Analysis-Example" class="headerlink" title="A Factor Analysis Example"></a>A Factor Analysis Example</h3><p>We have a  training data $ X_{n \times p} $. Here is its scatter plot. $ y = a $</p>
<p><img src="http://images.cnblogs.com/cnblogs_com/jerrylead/201105/201105111557474219.png" alt="plot"></p>
<ol>
<li>Generate a k dimension variable $F \sim N_k(0,I)$</li>
</ol>
<p><img src="http://images.cnblogs.com/cnblogs_com/jerrylead/201105/201105111557493007.png" alt="Factor"></p>
<ol>
<li>There exists a transformation matrix $L \in R^{p \times k}$ which maps F into n dimension space: $LF$</li>
</ol>
<p><img src="http://images.cnblogs.com/cnblogs_com/jerrylead/201105/20110511155750367.png" alt="transform"></p>
<ol>
<li>Add a mean $\mu$ on $LF$</li>
</ol>
<p><img src="http://images.cnblogs.com/cnblogs_com/jerrylead/201105/201105111557566675.png" alt="add_mu"></p>
<ol>
<li>For real  instance has errors, add error $\epsilon_{p \times 1}$</li>
</ol>
<p>$$X = LF+\mu + \epsilon$$</p>
<p><img src="http://images.cnblogs.com/cnblogs_com/jerrylead/201105/201105111558042959.png" alt="error"></p>
<h3 id="Factor-Analysis-Model"><a href="#Factor-Analysis-Model" class="headerlink" title="Factor Analysis Model"></a>Factor Analysis Model</h3><ul>
<li>Suppose $X \sim \Pi_p(\mu, \Sigma)$</li>
<li>The factor model postulates that $X$ is linearly related to a few unobservable random variables $F_1,F_2,&#x2026;,F_m$, called <strong>common factors</strong>&#xFF08;&#x5171;&#x540C;&#x56E0;&#x5B50;&#xFF09;, through</li>
</ul>

$$X- \mu = L_{p \times m} F_{m \times 1} + \epsilon_{p \times 1}$$

<p>where $L = (l_{ij})_{p \times m}$ is the matrix of <strong>factor loading</strong>&#xFF08;&#x56E0;&#x5B50;&#x8F7D;&#x8377;&#xFF09;, $l_{ij}$ is the loading of variable $i$ on factor $j$, $\epsilon = (\epsilon_1, . . . , \epsilon_p)&#x2032;$, $\epsilon_i$ are called errors or <strong>specific factors</strong>&#xFF08;&#x7279;&#x6B8A;&#x56E0;&#x5B50;&#xFF09;.</p>
<ul>
<li><strong>Assume</strong>: </li>
</ul>
<p>$$E(F) = 0, cov(F) = I_m, $$</p>
<p>$$E(\epsilon) = 0, cov(\epsilon) = \psi_{p \times p} = diag(\varphi_1,.., \varphi_p)$$</p>
<p>$$cov(F, \epsilon) = E(F \epsilon &#x2018;) = 0$$</p>
<p>Then</p>
<p>$$cov(X) = \Sigma_{p \times p} = LL&#x2019; + \psi$$</p>
<p>$$cov(X, F)  = L_{p \times m}$$</p>
<p>If $cov(F) \ne I_m$, it becomes oblique factor model&#xFF08;&#x659C;&#x4EA4;&#x56E0;&#x5B50;&#x6A21;&#x578B;&#xFF09;</p>
<ul>
<li>Define the $i_{th}$ <strong>community</strong>&#xFF08;&#x53D8;&#x91CF;&#x5171;&#x540C;&#x5EA6;&#xFF0C;&#x6216;&#x516C;&#x56E0;&#x5B50;&#x65B9;&#x5DEE;&#xFF09;: </li>
</ul>

$$h_i^2 = \sum_{j = 1}^m l_{ij}^2$$

<ul>
<li>Define the $i_{th}$ <strong>specific variance</strong>&#xFF08;&#x7279;&#x6B8A;&#x56E0;&#x5B50;&#x65B9;&#x5DEE;&#xFF09;:</li>
</ul>
$$\varphi_i = \sigma_{ii} - h_i^2$$ 
<h4 id="Ambiguity-of-L"><a href="#Ambiguity-of-L" class="headerlink" title="Ambiguity of L"></a>Ambiguity of L</h4><ul>
<li>Let T be any m &#xD7; m orthogonal matrix. Then, we can express</li>
</ul>
$$X- \mu = L^*F^* + \epsilon$$ 
<p>where $L^* = LT$, $F^* = T&apos;F$ </p>
<ul>
<li>Since $E(F^*) = 0$ , $cov(F^*) = I_{m}$ , $F^*$  and $L^*$  form another pair of factor and factor loading matrix.</li>
</ul>
$$ \Sigma = LL&apos; + \psi = L^* L&apos;^{*}  + \psi$$ 
$$h_i^2 = e_i&apos;LL&apos;e_i = e_i&apos;L^*L&apos;^*e_i$$ 
<p>After rotation, community $h_i^2$ doesn&#x2019;t change.</p>
<h2 id="3-Estimation"><a href="#3-Estimation" class="headerlink" title="3. Estimation"></a>3. Estimation</h2><h3 id="3-1-Principal-Component-Method"><a href="#3-1-Principal-Component-Method" class="headerlink" title="3.1 Principal Component Method"></a>3.1 Principal Component Method</h3><h4 id="1-Get-correlation-matrix"><a href="#1-Get-correlation-matrix" class="headerlink" title="1) Get correlation matrix"></a>1) Get correlation matrix</h4><p>$$\hat{Cor}(X) = \Sigma$$</p>
<h4 id="2-Spectral-Decompositions"><a href="#2-Spectral-Decompositions" class="headerlink" title="2) Spectral Decompositions"></a>2) Spectral Decompositions</h4><p>$$\Sigma = \lambda_1\ e_1e_1&#x2019;\ +\ &#x2026;\ +\ \lambda_p\ e_pe_p&#x2019;$$</p>
<h4 id="3-Determine-m"><a href="#3-Determine-m" class="headerlink" title="3) Determine $m$"></a>3) Determine $m$</h4><p>Rule of thumb: choose $m =\ \# \ of \{\lambda_j&gt;1\}$</p>
<h4 id="4-Estimation"><a href="#4-Estimation" class="headerlink" title="4) Estimation"></a>4) Estimation</h4><p>$$\hat L = (\sqrt{\lambda_1}\ e_1,\ &#x2026;\ ,\ \sqrt{\lambda_m}\ e_m)$$</p>
<p>$$\hat \psi = diag(\Sigma - LL&#x2019;)$$</p>
$$\hat h_i^2 = \sum_{j = 1}^m \hat l_{ij}^2$$
<p>The contribution to the total sample variance tr(S) from the first common factor is then&#xFF08;&#x516C;&#x5171;&#x56E0;&#x5B50;&#x7684;&#x65B9;&#x5DEE;&#x8D21;&#x732E;&#xFF09;</p>
$$\hat l^2_{11} + ...+ \hat l^2_{p1} = (\sqrt{\hat \lambda_1}\hat e_1)&apos;(\sqrt{\hat \lambda_1}\hat e_1) = \hat \lambda_1$$
<p>In general, the proportion of total sample variance(after standardization) due to the $j_{th}$ factor = $\frac{\hat \lambda_j}{p}$</p>
<h3 id="3-2-Maximum-Likelihood-Method"><a href="#3-2-Maximum-Likelihood-Method" class="headerlink" title="3.2 Maximum Likelihood Method"></a>3.2 Maximum Likelihood Method</h3><p><strong>1) Joint distribution:</strong></p>

$$
\begin{bmatrix}
 f\\
 x
 \end{bmatrix} \sim N \begin{pmatrix}
 \begin{bmatrix} 0\\
 \mu
 \end{bmatrix}, \begin{bmatrix}
 I &amp; L&apos;\\
 L &amp; LL&apos; + \psi
 \end{bmatrix}
 \end{pmatrix}$$

<p><strong>2) Marginal distribution:</strong><br>$$x \sim N(\mu, LL&#x2019;+\psi)$$<br><strong>3) Conditional distribution:</strong><br>$$\mu_{f|x} = L&#x2019;(LL&#x2019;+\psi)^{-1}(x-\mu)$$</p>
<p>$$\Sigma_{f|x} = I - L&#x2019;(LL&#x2019;+\psi)^{-1}L$$</p>
<p><strong>4) Log likelihood:</strong> </p>
<p>$$l(\mu, L, \psi) = log \prod_{i=1}^n \frac{1}{(2 \pi)^{p/2}|LL&#x2019;+\psi|} exp \left(-\frac{1}{2}(x^{(i)}-\mu)&#x2019;(LL&#x2019;+\psi)^{-1}(x^{(i)}-\mu)  \right)$$</p>
<h4 id="EM-estimation"><a href="#EM-estimation" class="headerlink" title="EM estimation"></a>EM estimation</h4><ul>
<li><strong>E Step:</strong></li>
</ul>
$$Q(f) = \frac{1}{(2 \pi)^{k/2}|\Sigma_{f|x}|} exp \left(-\frac{1}{2}(f-\mu_{f|x})&apos;(\Sigma_{f|x})^{-1}(x^{(i)}-\mu_{f|x})  \right)$$
<ul>
<li><strong>M Step:</strong></li>
</ul>
$$max\ \ \sum_{i=1}^n \int_{f^{(i)}} Q(f^{(i)})log \frac{p(x^{(i)}&#xFF0C;f^{(i)};\mu, L, \psi)}{Q(f^{(i)})} $$
<ul>
<li><strong>Parameter Iteration:</strong></li>
</ul>
<p><img src="http://images.cnblogs.com/cnblogs_com/jerrylead/201105/201105111558444306.png" alt="L est"></p>
<p><img src="http://images.cnblogs.com/cnblogs_com/jerrylead/201105/201105111558474881.png" alt="mu est"></p>
<p><img src="http://images.cnblogs.com/cnblogs_com/jerrylead/201105/201105111558484749.jpg" alt="psi est"></p>
<p>$$\psi = diag(\Phi)$$</p>
<p>Get more detail on <a href="http://blog.csdn.net/littleqqqqq/article/details/50899717" target="_blank" rel="external">&#x3010;&#x673A;&#x5668;&#x5B66;&#x4E60;-&#x65AF;&#x5766;&#x798F;&#x3011;&#x56E0;&#x5B50;&#x5206;&#x6790;&#xFF08;Factor Analysis&#xFF09; </a></p>
<h2 id="4-Factor-Rotation"><a href="#4-Factor-Rotation" class="headerlink" title="4. Factor Rotation"></a>4. Factor Rotation</h2><p>An orthogonal matrix $T$, and let $L^* = LT$.</p>
<ul>
<li><p><strong>Goal: </strong>to rotate $L$ such that a &#x2018;simple&#x2019; structure is achieved.</p>
</li>
<li><p>Kaiser (1958)&#x2019;s <strong>varimax</strong> criterion&#xFF08;&#x65B9;&#x5DEE;&#x6700;&#x5927;&#x65CB;&#x8F6C;&#xFF09; :</p>
<ul>
<li>Define $\widetilde l^*_ {ij} = \hat l^*_{ij}/h_i^2$</li>
<li>Choose $T$ s.t.</li>
</ul>
</li>
</ul>
$$max\ \ V=\frac{1}{p} \sum_{j=1}^m \left ({\sum_{i=1}^p {\widetilde l^*_ {ij}}^4 - \frac{\left(\sum_{i = 1}^p {\widetilde l^*_ {ij}}^2 \right)^2}{p} }\right )$$
<h2 id="5-Factor-Scores"><a href="#5-Factor-Scores" class="headerlink" title="5. Factor Scores"></a>5. Factor Scores</h2><h3 id="Weighted-Least-Squares-Method"><a href="#Weighted-Least-Squares-Method" class="headerlink" title="Weighted Least Squares Method"></a>Weighted Least Squares Method</h3><ul>
<li>Suppose that $\mu$, $L$, and $\psi$ are known.</li>
<li>Then $X-\mu = LF + \epsilon \sim \Pi_p(0, \psi)$</li>
</ul>
<p>$$\hat F = (L&#x2019; \psi ^{-1}L)^{-1}L&#x2019; \psi^{-1} (X-\mu)$$</p>
<h3 id="Regression-Method"><a href="#Regression-Method" class="headerlink" title="Regression Method"></a>Regression Method</h3><p>From the mean of the conditional distribution of $F|X$ is $\mu_{f|x} = L&#x2019;(LL&#x2019;+\psi)^{-1}(x-\mu)$</p>
<p>$$\hat F = \hat E(F|X) = L&#x2019;\Sigma^{-1}(X-\overline X)$$</p>

  </section>

  <section class="post-comments">

    <!-- 将评论系统（例如Disqus、多说、友言、畅言等）提供的代码片段粘贴在这里 -->
    
</section>


</article>


            <footer class="footer">

    <span class="footer__copyright">&copy; 2014-2015. | 由<a href="https://hexo.io/">Hexo</a>强力驱动 | 主题<a href="https://github.com/someus/huno">Huno</a></span>
    
</footer>
        </div>
    </div>

    <!-- js files -->
    <script src="/teaching_ml/js/jquery.min.js"></script>
    <script src="/teaching_ml/js/main.js"></script>
    <script src="/teaching_ml/js/scale.fix.js"></script>
    

    

    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript"> 
        $(document).ready(function(){
            MathJax.Hub.Config({ 
                tex2jax: {inlineMath: [['[latex]','[/latex]'], ['\\(','\\)']]} 
            });
        });
    </script>


    

    <script src="/teaching_ml/js/awesome-toc.min.js"></script>
    <script>
        $(document).ready(function(){
            $.awesome_toc({
                overlay: true,
                contentId: "post-content",
            });
        });
    </script>


    
    
    <!--kill ie6 -->
<!--[if IE 6]>
  <script src="//letskillie6.googlecode.com/svn/trunk/2/zh_CN.js"></script>
<![endif]-->

</body>
</html>
