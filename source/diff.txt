diff --git a/package.json b/package.json
index 06a7fbd..6ea745a 100644
--- a/package.json
+++ b/package.json
@@ -3,7 +3,7 @@
   "version": "0.0.0",
   "private": true,
   "hexo": {
-    "version": "3.2.2"
+    "version": "3.3.8"
   },
   "dependencies": {
     "hexo": "^3.2.0",
@@ -18,4 +18,4 @@
     "hexo-renderer-stylus": "^0.3.1",
     "hexo-server": "^0.2.0"
   }
-}
+}
\ No newline at end of file
diff --git a/source/_posts/Word2vec.md b/source/_posts/Word2vec.md
new file mode 100644
index 0000000..f87659d
--- /dev/null
+++ b/source/_posts/Word2vec.md
@@ -0,0 +1,88 @@
+---
+title: '[Word2vec]'
+date: 2017-08-11 15:38:07
+tags:
+---
+
+# Word2Vec
+
+## Outline
++ 统计语言模型
++ 神经概率语言模型
++ 基于Hierarchical Softmax的CBOW模型
++ 基于Negative Sampling的Skip-gram模型
+
+### 统计语言模型
++ __定义__:给定一个特定顺序的词串，统计语言模型计算该词串是一个有意义的句子的概率
+
+    p(w 1 , w 2 ,..., w t )=p(w 1 )·p(w 2 |w 1 )· ... ·p(w t |w 1 , w 2 ,..., w t-1 )
++ __例子__:
+
+    p("Today is Friday")≈0.001 > p("Today Friday is")≈0.00000001
++ __复杂度估计__:
+  - 假设词典大小为N,句子的长度为t,则共有N t 种组合。每一种组合包含t个参
+数,总共需要计算和存储t·N t 个参数。
+
+#### N-Gram语言模型
++ __基本思想__:一个词出现的概率只与其前面n-1个词相关
+
+  p(w k |w 1 ,w 2 ...w k-1 )≈p(w k |w k-n+1 ,w k-n+2 ,...,w k-1 )
+
+  =p(w k-n+1 ,w k-n+2 ,...,w k )/p(w k-n+1 ,w k-n+2 ,...,w k-1 )
+
+  ≈count(w k-n+1 ,w k-n+2 ,...,w k )/count(w k-n+1 ,w k-n+2 ,...,w k-1 )
++ __平滑化问题__:
+  - 若count(w k-n+1 ,w k-n+2 ,...,w k )=0,能否认为p(w k |w 1 ,w 2 ...w k-1 )=0?
+  - 若count(w k-n+1 ,w k-n+2 ,...,w k-1 )=count(w k-n+1 ,w k-n+2 ,...,w k ),能否认为
+p(w k |w 1 ,w 2 ...w k-1 )=1?
+### 神经概率语言模型
+![alt text](Word2vec/1.png)
+
+    其中,是词w的输出向量(长度为N),i_w是词w在词典中的位置,y_w(i_w)是输
+出向量y_w上位于i_w的元素,N是词典的大小
+#### 词向量比较
++ One-hot representation
+
+__定义__:词向量的大小与词典大小相同,词向量中,只有该词对应位置的元素为1,其余为零
+  - 优点:简单
+  - 缺点:语义鸿沟,维数灾难
+  - Distributed representation、
+
+__定义__:低维稠密实向量
+  - 优点:具有语义、语法相似性
+  - 缺点:缺乏解释性,计算复杂
+
+![alt text](Word2vec/2.png)
+
++ *复杂度估计*
+ + w的上下文词的数量,通常不超过5
+ + 词向量的长度,通常是10 1 -10 2 量级
+ + 隐藏层神经元数量,通常是10 2 量级
+ + 词典的大小N,通常是10 4 -10 5 量级
+
+  整个模型的大部分计算集中在隐藏层和输出层的矩阵向量计算以及输出层中
+  的softmax归一化!
+### 基于Hierarchical Softmax的CBOW模型
++ 输入层:包含Context(w)中2c个词的词向量
++ 投影层:将输入层中2c个词向量求和累加
++ 输出层:是一棵二叉树,由词典中的词作为叶子节点,以各词在语料中出现的次数作为权值,构建出来的一棵Huffman树
+
+![alt text](Word2vec/3.png)
+
+Quiz：CBOW模型和神经概率语言模型相比,结构有什么区别?
+
+![alt text](Word2vec/4.png)
+
+将Huffman编码为1的结点定义为负类,将编码为0的结点定义为正类,即
+
+![alt text](Word2vec/5.png)
+
+易知,一个结点被分为正类的概率是![alt text](Word2vec/6.png),被分为负类的概率是![alt text](Word2vec/7.png)
+
+对于从根结点出发到“足球”叶子结点的4次二分类,每次分类结果的概率是
+
+![alt text](Word2vec/8.png)
+
+
+
+
diff --git a/source/_posts/Word2vec/1.png b/source/_posts/Word2vec/1.png
new file mode 100644
index 0000000..20f9620
Binary files /dev/null and b/source/_posts/Word2vec/1.png differ
diff --git a/source/_posts/Word2vec/2.png b/source/_posts/Word2vec/2.png
new file mode 100644
index 0000000..9054d39
Binary files /dev/null and b/source/_posts/Word2vec/2.png differ
diff --git a/source/_posts/Word2vec/3.png b/source/_posts/Word2vec/3.png
new file mode 100644
index 0000000..ec0bffe
Binary files /dev/null and b/source/_posts/Word2vec/3.png differ
diff --git a/source/_posts/Word2vec/4.png b/source/_posts/Word2vec/4.png
new file mode 100644
index 0000000..cf4f4c0
Binary files /dev/null and b/source/_posts/Word2vec/4.png differ
diff --git a/source/_posts/Word2vec/5.png b/source/_posts/Word2vec/5.png
new file mode 100644
index 0000000..9e1fcf9
Binary files /dev/null and b/source/_posts/Word2vec/5.png differ
diff --git a/source/_posts/Word2vec/6.png b/source/_posts/Word2vec/6.png
new file mode 100644
index 0000000..ba705d9
Binary files /dev/null and b/source/_posts/Word2vec/6.png differ
diff --git a/source/_posts/Word2vec/7.png b/source/_posts/Word2vec/7.png
new file mode 100644
index 0000000..24aeb21
Binary files /dev/null and b/source/_posts/Word2vec/7.png differ
diff --git a/source/_posts/Word2vec/8.png b/source/_posts/Word2vec/8.png
new file mode 100644
index 0000000..ecb4363
Binary files /dev/null and b/source/_posts/Word2vec/8.png differ
diff --git a/source/_posts/test.md b/source/_posts/test.md
new file mode 100644
index 0000000..5e207ba
--- /dev/null
+++ b/source/_posts/test.md
@@ -0,0 +1,9 @@
+---
+title: '[test]'
+date: 2017-08-09 17:35:22
+tags:
+---
+
+#小朱的文档上传测试
+
+##马上删
diff --git a/source/index.md b/source/index.md
index 8fbab87..a57005d 100644
--- a/source/index.md
+++ b/source/index.md
@@ -4,16 +4,16 @@
 
 # 常见机器学习算法
 ## 已分享
-1. [SVM](2016/08/30/svm/)
-2. [关联规则](2016/07/04/associations/%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E6%8C%96%E6%8E%98%E5%9F%BA%E7%A1%80%E7%AF%87/)
+1. [SVM](https://github.com/transwarpio/teaching_ml/blob/master/source/_posts/svm.org)
+2. [关联规则](https://github.com/transwarpio/teaching_ml/blob/master/source/_posts/associations/%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E6%8C%96%E6%8E%98%E5%9F%BA%E7%A1%80%E7%AF%87.md)
 3. [ALS](https://github.com/endymecy/spark-ml-source-analysis/blob/master/%E6%8E%A8%E8%8D%90/ALS.md)
 4. [LDA](https://github.com/endymecy/spark-ml-source-analysis/blob/master/%E8%81%9A%E7%B1%BB/LDA/lda.md)
 5. [Gaussian Mixture](https://github.com/endymecy/spark-ml-source-analysis/blob/master/%E8%81%9A%E7%B1%BB/gaussian-mixture/gaussian-mixture.md)
 6. [Bistecting KMeans](https://github.com/endymecy/spark-ml-source-analysis/blob/master/%E8%81%9A%E7%B1%BB/bis-k-means/bisecting-k-means.md)
 7. [KMeans](https://github.com/endymecy/spark-ml-source-analysis/blob/master/%E8%81%9A%E7%B1%BB/k-means/k-means.md)
 8. [PIC](https://github.com/endymecy/spark-ml-source-analysis/blob/master/%E8%81%9A%E7%B1%BB/PIC/pic.md)
-9. [Factor Analysis](2016/12/01/FactorAnalysis)
-
+9. [Factor Analysis](https://github.com/transwarpio/teaching_ml/blob/master/source/_posts/FactorAnalysis.md)
+10. [test](https://github.com/transwarpio/teaching_ml/blob/master/source/_posts/test.md)
 ## 将分享
 5. Logistic Regression
 6. Decision Tree
@@ -44,7 +44,7 @@
 ## 已分享
 1. [MXNet框架从原理到代码](2016/07/05/mxnet)
 2. [深度信念网络在蛋白质突变检测中的应用](https://github.com/xzry6/notes/blob/master/transwarp/dbn.md)
-
+3. [Word2Vec](https://github.com/transwarpio/teaching_ml/blob/master/source/_posts/Word2vec.md)
 ## 将分享
 1. tensor,conv,pooling
 2. Word2Vec
