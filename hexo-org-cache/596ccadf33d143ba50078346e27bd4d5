{"md5":"df5a8e2084c3a97cb87e4a84df85f424","content":"\n\n\n<div id=\"outline-container-sec-1\" class=\"outline-2\">\n<h2 id=\"sec-1\"><span class=\"section-number-2\">1</span> SVM</h2>\n<div class=\"outline-text-2\" id=\"text-1\">\n</div><div id=\"outline-container-sec-1-1\" class=\"outline-3\">\n<h3 id=\"sec-1-1\"><span class=\"section-number-3\">1.1</span> 介绍</h3>\n<div class=\"outline-text-3\" id=\"text-1-1\">\n<p>\nSupport Vector Machine 支持向量机是一种机器学习算法。 \n</p>\n\n<p>\n给定一个训练集 \\( S = \\{ (x_i, y_i) \\}_{i=1}^{m} \\), 其中 \\( x_i \\in \\mathbb{R}^n \\) 并且 \\( y_i \\in \\{ +1, -1 \\} \\),\n图<a href=\"#svm\">1</a>展示了一个 SVM 需要解决的问题。 我们标记  \\( w \\cdot x - b = 0 \\) 为超平面， \\( w \\) 代表该超平面的向量。 \n我们需要做的是找到能将 \\( y_i=1 \\) 的点和 \\( y_i=-1 \\) 的点 分开的边际最大的超平面.\n这就意味着 \\( y_i(w \\cdot x_i -b ) \\geq 1 \\)，对于所有 \\( 1 \\leq i \\leq n \\).\n</p>\n\n<p>\n所以优化问题可以写成：\n</p>\n\n<p>\n最大化\n</p>\n\n<p>\n\\[ \\frac{2}{\\|w\\|} \\]\n</p>\n\n<p>\n这等价于最小化\n</p>\n\n<p>\n\\[ \\frac{1}{2} \\| w \\|^2 \\]\n</p>\n\n<p>\nsubject to \\( y_i(w \\cdot x_i - b) \\geq 1 \\) for all \\( 1 \\leq i \\leq n \\)\n</p>\n\n\n<div id=\"svm\" class=\"figure\">\n<p><img src=\"images/svm/svm.png\" alt=\"captionm\" width=\"400px\">\n</p>\n<p><span class=\"figure-number\">Figure 1:</span> SVM</p>\n</div>\n\n<p>\n事实上，它可以被看作一个带有惩罚项的最小化损失问题。最终，我们希望找到以下问题的最小解\n</p>\n\n<p>\n\\[\n \\min_{w} \\frac{\\lambda}{2}\\|w\\|^2 + \\frac{1}{m}\\sum_{(x, y) \\in S} \\ell(w; (x, y))\n\\]\n</p>\n\n<p>\n其中 &lambda; 是正规化参数, \\( \\ell(w, (x, y)) \\) 是 hinge 损失函数:\n</p>\n\n<p>\n\\[\n\\ell(w, (x, y)) = max\\{0, 1-y \\langle w, x \\rangle \\}\n\\]\n</p>\n\n<p>\n对于这一最优化问题，我们可以使用梯度下降算法来达到最小值。\n</p>\n\n<p>\n目标函数为：\n</p>\n\n<p>\n\\[\nf(w) = \\frac{\\lambda}{2}\\|w\\|^2 + \\frac{1}{m}\\sum_{(x_i, y_i) \\in S}\\ell(w; (x_i, y_i))\n\\]\n</p>\n\n<p>\n所以，迭代 <i>t</i> 时的梯度为：\n</p>\n\n<p>\n\\[\n\\nabla_t = \\lambda w_t - \\frac{1}{m}\\sum_{(x_i, y_i) \\in S}\\mathbbm{1}[y_i \\langle w, x_i \\rangle < 1]y_i x_i\n\\]\n</p>\n\n<p>\n于是，我们可以更新  \\( w \\), 其中 \\( \\eta_t \\) 是下降速度\n\\[\nw_{t+1} \\leftarrow w_t - \\eta_t\\nabla_t\n\\]\n</p>\n</div>\n</div>\n\n<div id=\"outline-container-sec-1-2\" class=\"outline-3\">\n<h3 id=\"sec-1-2\"><span class=\"section-number-3\">1.2</span> SGD</h3>\n<div class=\"outline-text-3\" id=\"text-1-2\">\n<p>\n从上一节我们可以看到每次迭代我们都需要所有的数据点来计算梯度。而当数据集变大后，无疑会耗费大量的计算时间。\n这就是为什么在大规模梯度下降算法中，我们总会使用 SGD（随机梯度下降）。SDG 在每次迭代时只使用一部分数据而不是全部，\n从而降低了计算量。\n</p>\n\n<p>\n所以，现在目标函数变成了：\n\\[\nf(w, A_t) = \\frac{\\lambda}{2}\\|w\\|^2 + \\frac{1}{k}\\sum_{(x_i, y_i) \\in A_t}\\ell(w; (x_i, y_i))\n\\]\nwhere \\( A_t \\subset S \\), \\( |A_t| = k \\). At each iteration, we takes a subset of data point.\n</p>\n\n<p>\n然后梯度为：\n \\[ \\nabla_t = \\lambda w_t - \\frac{1}{k}\\sum_{(x_i, y_i) \\in A_t}\\mathbbm{1}[y_i \\langle w, x_i \\rangle < 1]y_i x_i \\]\n</p>\n</div>\n</div>\n\n<div id=\"outline-container-sec-1-3\" class=\"outline-3\">\n<h3 id=\"sec-1-3\"><span class=\"section-number-3\">1.3</span> Pegasos and MLlib implementation</h3>\n<div class=\"outline-text-3\" id=\"text-1-3\">\n<p>\nPegasos 是 SVM 使用梯度下降算法的一种实现。Spark MLlib 也提供了 SVM 的梯度下降实现，于 Pegasos 稍有不同。\n主要是梯度的更新速度不同。\n</p>\n\n<p>\n\\[\nw_{t+1} \\leftarrow w_t - \\eta_t\\nabla_t\n\\]\n</p>\n\n<p>\n在 Pegasos 算法中, 更新速度为\n\\[\n\\eta_t = \\frac{\\alpha}{t\\lambda}\n\\]\n</p>\n\n<p>\n而在 MLlib 中，为：\n\\[\n\\eta_t = \\frac{\\alpha}{\\sqrt{t}}\n\\]\n</p>\n\n<p>\n其中 &alpha; 是更新速度参数。\n</p>\n</div>\n</div>\n</div>\n\n<div id=\"outline-container-sec-2\" class=\"outline-2\">\n<h2 id=\"sec-2\"><span class=\"section-number-2\">2</span> SGD in Spark</h2>\n<div class=\"outline-text-2\" id=\"text-2\">\n</div><div id=\"outline-container-sec-2-1\" class=\"outline-3\">\n<h3 id=\"sec-2-1\"><span class=\"section-number-3\">2.1</span> treeAggregate</h3>\n<div class=\"outline-text-3\" id=\"text-2-1\">\n<p>\nSpark 来计算 SGD 的主要优势使可以分布式地计算梯度，然后将它们累加起来。\n在 Spark 中，这一任务是通过 RDD 的 <b>treeAggregate</b> 方法来完成的。\n<b>Aggregate</b> 可被视为泛化的 <b>Map</b> 和 <b>Reduce</b> 的组合。 <b>treeAggregate</b> 的定义为\n</p>\n\n<div class=\"org-src-container\">\n\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">RDD</span>.treeAggregate(zeroValue: <span class=\"type\">U</span>)(</span><br><span class=\"line\">      seqOp: (<span class=\"type\">U</span>, <span class=\"type\">T</span>) =&amp;gt; <span class=\"type\">U</span>,</span><br><span class=\"line\">      combOp: (<span class=\"type\">U</span>, <span class=\"type\">U</span>) =&amp;gt; <span class=\"type\">U</span>,</span><br><span class=\"line\">      depth: <span class=\"type\">Int</span> = <span class=\"number\">2</span>): <span class=\"type\">U</span></span><br></pre></td></tr></table></figure>\n</div>\n\n<p>\n在此方法中有三个参数，其中前两个对我们更重要：\n</p>\n\n<ul class=\"org-ul\">\n<li>seqOp: 计算每隔 partition 中的子梯度\n</li>\n<li>combOp: 将 seqOp 或上层 combOp 的值合并\n</li>\n<li>depth: 控制 tree 的深度\n</li>\n</ul>\n\n\n<div id=\"tree\" class=\"figure\">\n<p><img src=\"images/svm/tree.png\" alt=\"tree.png\">\n</p>\n<p><span class=\"figure-number\">Figure 2:</span> tree aggregate</p>\n</div>\n</div>\n</div>\n\n<div id=\"outline-container-sec-2-2\" class=\"outline-3\">\n<h3 id=\"sec-2-2\"><span class=\"section-number-3\">2.2</span> 实现</h3>\n<div class=\"outline-text-3\" id=\"text-2-2\">\n<p>\nSGD 是一个求最优化的算法，许多机器学习算法都可以用 SGD 来求解。所以 Spark 对其做了抽象。\n</p>\n\n<div class=\"org-src-container\">\n\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">SVMWithSGD</span> <span class=\"title\">private</span> (</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">var</span> stepSize: <span class=\"type\">Double</span>,</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">var</span> numIterations: <span class=\"type\">Int</span>,</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">var</span> regParam: <span class=\"type\">Double</span>,</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">var</span> miniBatchFraction: <span class=\"type\">Double</span>)</span><br><span class=\"line\">  <span class=\"keyword\">extends</span> <span class=\"type\">GeneralizedLinearAlgorithm</span>[<span class=\"type\">SVMModel</span>] <span class=\"keyword\">with</span> <span class=\"type\">Serializable</span> {</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">val</span> gradient = <span class=\"keyword\">new</span> <span class=\"type\">HingeGradient</span>()</span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">val</span> updater = <span class=\"keyword\">new</span> <span class=\"type\">SquaredL2Updater</span>()</span><br><span class=\"line\">  <span class=\"annotation\">@Since</span>(<span class=\"string\">\"0.8.0\"</span>)</span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"keyword\">val</span> optimizer = <span class=\"keyword\">new</span> <span class=\"type\">GradientDescent</span>(gradient, updater)</span><br><span class=\"line\">    .setStepSize(stepSize)</span><br><span class=\"line\">    .setNumIterations(numIterations)</span><br><span class=\"line\">    .setRegParam(regParam)</span><br><span class=\"line\">    .setMiniBatchFraction(miniBatchFraction)</span><br></pre></td></tr></table></figure>\n</div>\n\n<p>\n可以看到 <code>SVMWithSGD</code> 继承了 <code>GeneralizedLinearAlgorithm</code> ，并定义 <code>optimizer</code> 来确定如何获得优化解。\n而 <code>optimizer</code> 即是 SGD 算法的实现。正如上节所述，线性 SVM 实际上是使用 hinge 损失函数和一个 L2 惩罚项的线性模型，因此这里使用了 <code>HingeGradient</code> 和 <code>SquaredL2Updater</code> \n作为 <code>GradientDescent</code> 的参数。\n</p>\n\n<div class=\"org-src-container\">\n\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">HingeGradient</span> <span class=\"keyword\"><span class=\"keyword\">extends</span></span> <span class=\"title\">Gradient</span> {</span></span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compute</span>(</span>data: <span class=\"type\">Vector</span>, label: <span class=\"type\">Double</span>, weights: <span class=\"type\">Vector</span>): (<span class=\"type\">Vector</span>, <span class=\"type\">Double</span>) = {</span><br><span class=\"line\">    <span class=\"keyword\">val</span> dotProduct = dot(data, weights)</span><br><span class=\"line\">    <span class=\"comment\">// Our loss function with {0, 1} labels is max(0, 1 - (2y - 1) (f_w(x)))</span></span><br><span class=\"line\">    <span class=\"comment\">// Therefore the gradient is -(2y - 1)*x</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> labelScaled = <span class=\"number\">2</span> * label - <span class=\"number\">1.0</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (<span class=\"number\">1.0</span> &amp;gt; labelScaled * dotProduct) {</span><br><span class=\"line\">      <span class=\"keyword\">val</span> gradient = data.copy</span><br><span class=\"line\">      scal(-labelScaled, gradient)</span><br><span class=\"line\">      (gradient, <span class=\"number\">1.0</span> - labelScaled * dotProduct)</span><br><span class=\"line\">    } <span class=\"keyword\">else</span> {</span><br><span class=\"line\">      (<span class=\"type\">Vectors</span>.sparse(weights.size, <span class=\"type\">Array</span>.empty, <span class=\"type\">Array</span>.empty), <span class=\"number\">0.0</span>)</span><br><span class=\"line\">    }</span><br><span class=\"line\">  }</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compute</span>(</span></span><br><span class=\"line\">      data: <span class=\"type\">Vector</span>,</span><br><span class=\"line\">      label: <span class=\"type\">Double</span>,</span><br><span class=\"line\">      weights: <span class=\"type\">Vector</span>,</span><br><span class=\"line\">      cumGradient: <span class=\"type\">Vector</span>): <span class=\"type\">Double</span> = {</span><br><span class=\"line\">    <span class=\"keyword\">val</span> dotProduct = dot(data, weights)</span><br><span class=\"line\">    <span class=\"comment\">// Our loss function with {0, 1} labels is max(0, 1 - (2y - 1) (f_w(x)))</span></span><br><span class=\"line\">    <span class=\"comment\">// Therefore the gradient is -(2y - 1)*x</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> labelScaled = <span class=\"number\">2</span> * label - <span class=\"number\">1.0</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (<span class=\"number\">1.0</span> &amp;gt; labelScaled * dotProduct) {</span><br><span class=\"line\">      axpy(-labelScaled, data, cumGradient)</span><br><span class=\"line\">      <span class=\"number\">1.0</span> - labelScaled * dotProduct</span><br><span class=\"line\">    } <span class=\"keyword\">else</span> {</span><br><span class=\"line\">      <span class=\"number\">0.0</span></span><br><span class=\"line\">    }</span><br><span class=\"line\">  }</span><br><span class=\"line\">}</span><br></pre></td></tr></table></figure>\n</div>\n\n<div class=\"org-src-container\">\n\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/**</span><br><span class=\"line\"> * :: DeveloperApi ::</span><br><span class=\"line\"> * Updater for L2 regularized problems.</span><br><span class=\"line\"> *          R(w) = 1/2 ||w||^2</span><br><span class=\"line\"> * Uses a step-size decreasing with the square root of the number of iterations.</span><br><span class=\"line\"> */</span></span><br><span class=\"line\"><span class=\"annotation\">@DeveloperApi</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">SquaredL2Updater</span> <span class=\"keyword\"><span class=\"keyword\">extends</span></span> <span class=\"title\">Updater</span> {</span></span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compute</span>(</span></span><br><span class=\"line\">      weightsOld: <span class=\"type\">Vector</span>,</span><br><span class=\"line\">      gradient: <span class=\"type\">Vector</span>,</span><br><span class=\"line\">      stepSize: <span class=\"type\">Double</span>,</span><br><span class=\"line\">      iter: <span class=\"type\">Int</span>,</span><br><span class=\"line\">      regParam: <span class=\"type\">Double</span>): (<span class=\"type\">Vector</span>, <span class=\"type\">Double</span>) = {</span><br><span class=\"line\">    <span class=\"comment\">// add up both updates from the gradient of the loss (= step) as well as</span></span><br><span class=\"line\">    <span class=\"comment\">// the gradient of the regularizer (= regParam * weightsOld)</span></span><br><span class=\"line\">    <span class=\"comment\">// w' = w - thisIterStepSize * (gradient + regParam * w)</span></span><br><span class=\"line\">    <span class=\"comment\">// w' = (1 - thisIterStepSize * regParam) * w - thisIterStepSize * gradient</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> thisIterStepSize = stepSize / math.sqrt(iter)</span><br><span class=\"line\">    <span class=\"keyword\">val</span> brzWeights: <span class=\"type\">BV</span>[<span class=\"type\">Double</span>] = weightsOld.asBreeze.toDenseVector</span><br><span class=\"line\">    brzWeights :*= (<span class=\"number\">1.0</span> - thisIterStepSize * regParam)</span><br><span class=\"line\">    brzAxpy(-thisIterStepSize, gradient.asBreeze, brzWeights)</span><br><span class=\"line\">    <span class=\"keyword\">val</span> norm = brzNorm(brzWeights, <span class=\"number\">2.0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    (<span class=\"type\">Vectors</span>.fromBreeze(brzWeights), <span class=\"number\">0.5</span> * regParam * norm * norm)</span><br><span class=\"line\">  }</span><br><span class=\"line\">}</span><br></pre></td></tr></table></figure>\n</div>\n\n<p>\n此节中, <a href=\"#code\">1</a> 展示了 <code>GradientDescent</code> 的主要执行逻辑。 重复执行 <code>numIterations</code> 次以获得最终的 \\( w \\)。\n</p>\n\n<p>\n首先, <code>data.sample</code> 通过 <code>miniBatchFraction</code> 取一部分样本. 然后使用 <code>treeAggregate</code> 。\n在 <code>seqOp</code> 中, <code>gradientSum</code> 会通过 <code>axpy(y, b_x, c._1)</code> 更新，如果 \\( y\\langle w, x \\rangle < 1 \\)，即分类错误。\n在 <code>combOp</code> 中, <code>gradientSum</code> 通过 <code>c1._1 += c2._1</code> 被集合起来。 当获得 <code>gradientSum</code> 后, 我们就可以计算 <code>step</code> 和 <code>gradient</code> 了。\n最后, 我们使用 <code>axpy(-step, gradient, weights)</code> 更新 <code>weights</code> 。\n</p>\n\n<div class=\"org-src-container\">\n<label class=\"org-src-name\">GradientDescent 代码片断</label>\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">while</span> (!converged &amp;amp;&amp;amp; i &amp;lt;= numIterations) {</span><br><span class=\"line\">  <span class=\"keyword\">val</span> bcWeights = data.context.broadcast(weights)</span><br><span class=\"line\">  <span class=\"comment\">// Sample a subset (fraction miniBatchFraction) of the total data</span></span><br><span class=\"line\">  <span class=\"comment\">// compute and sum up the subgradients on this subset (this is one map-reduce)</span></span><br><span class=\"line\">  <span class=\"keyword\">val</span> (gradientSum, lossSum, miniBatchSize) = data.sample(<span class=\"literal\">false</span>, miniBatchFraction, <span class=\"number\">42</span> + i)</span><br><span class=\"line\">    .treeAggregate((<span class=\"type\">BDV</span>.zeros[<span class=\"type\">Double</span>](n), <span class=\"number\">0.0</span>, <span class=\"number\">0</span>L))(</span><br><span class=\"line\">      seqOp = (c, v) =&amp;gt; {</span><br><span class=\"line\">\t<span class=\"comment\">// c: (grad, loss, count), v: (label, features)</span></span><br><span class=\"line\">\t<span class=\"keyword\">val</span> l = gradient.compute(v._2, v._1, bcWeights.value, <span class=\"type\">Vectors</span>.fromBreeze(c._1))</span><br><span class=\"line\">\t(c._1, c._2 + l, c._3 + <span class=\"number\">1</span>)</span><br><span class=\"line\">      },</span><br><span class=\"line\">      combOp = (c1, c2) =&amp;gt; {</span><br><span class=\"line\">\t<span class=\"comment\">// c: (grad, loss, count)</span></span><br><span class=\"line\">\t(c1._1 += c2._1, c1._2 + c2._2, c1._3 + c2._3)</span><br><span class=\"line\">      })</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">if</span> (miniBatchSize &amp;gt; <span class=\"number\">0</span>) {</span><br><span class=\"line\">    <span class=\"comment\">/**</span><br><span class=\"line\">     * lossSum is computed using the weights from the previous iteration</span><br><span class=\"line\">     * and regVal is the regularization value computed in the previous iteration as well.</span><br><span class=\"line\">     */</span></span><br><span class=\"line\">    stochasticLossHistory.append(lossSum / miniBatchSize + regVal)</span><br><span class=\"line\">    <span class=\"keyword\">val</span> update = updater.compute(</span><br><span class=\"line\">      weights, <span class=\"type\">Vectors</span>.fromBreeze(gradientSum / miniBatchSize.toDouble),</span><br><span class=\"line\">      stepSize, i, regParam)</span><br><span class=\"line\">    weights = update._1</span><br><span class=\"line\">    regVal = update._2</span><br><span class=\"line\"></span><br><span class=\"line\">    previousWeights = currentWeights</span><br><span class=\"line\">    currentWeights = <span class=\"type\">Some</span>(weights)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (previousWeights != <span class=\"type\">None</span> &amp;amp;&amp;amp; currentWeights != <span class=\"type\">None</span>) {</span><br><span class=\"line\">      converged = isConverged(previousWeights.get,</span><br><span class=\"line\">\tcurrentWeights.get, convergenceTol)</span><br><span class=\"line\">    }</span><br><span class=\"line\">  } <span class=\"keyword\">else</span> {</span><br><span class=\"line\">    logWarning(s<span class=\"string\">\"Iteration ($i/$numIterations). The size of sampled batch is zero\"</span>)</span><br><span class=\"line\">  }</span><br><span class=\"line\">  i += <span class=\"number\">1</span></span><br></pre></td></tr></table></figure>\n</div>\n</div>\n</div>\n</div>\n\n<div id=\"outline-container-sec-3\" class=\"outline-2\">\n<h2 id=\"sec-3\"><span class=\"section-number-2\">3</span> 实验和性能</h2>\n<div class=\"outline-text-2\" id=\"text-3\">\n</div><div id=\"outline-container-sec-3-1\" class=\"outline-3\">\n<h3 id=\"sec-3-1\"><span class=\"section-number-3\">3.1</span> 正确性验证</h3>\n<div class=\"outline-text-3\" id=\"text-3-1\">\n<p>\n我们模拟了一些简单的 2D 和 3D 数据来验证正确性。\n</p>\n\n<div id=\"2d-linear\" class=\"figure\">\n<p><img src=\"images/svm/2d_linear.png\" alt=\"2d_linear.png\">\n</p>\n<p><span class=\"figure-number\">Figure 3:</span> 2D linear</p>\n</div>\n\n\n<div id=\"3d-linear\" class=\"figure\">\n<p><img src=\"images/svm/3d_linear.png\" alt=\"3d_linear.png\">\n</p>\n<p><span class=\"figure-number\">Figure 4:</span> 3D linear</p>\n</div>\n</div>\n</div>\n\n<div id=\"outline-container-sec-3-2\" class=\"outline-3\">\n<h3 id=\"sec-3-2\"><span class=\"section-number-3\">3.2</span> 收敛速度</h3>\n<div class=\"outline-text-3\" id=\"text-3-2\">\n<p>\n我们比较两种实现的收敛速度差异。这里，我们使用 5GB 带有 1000 个特征的模拟数据。使用 4 个 executors 并迭代 100 次。\n</p>\n\n\n<div id=\"convergence1\" class=\"figure\">\n<p><img src=\"images/svm/step1.png\" alt=\"step1.png\">\n</p>\n<p><span class=\"figure-number\">Figure 5:</span> before aligning Y axis</p>\n</div>\n\n\n<div id=\"convergence2\" class=\"figure\">\n<p><img src=\"images/svm/step2.png\" alt=\"step2.png\">\n</p>\n<p><span class=\"figure-number\">Figure 6:</span> after aligning Y axis</p>\n</div>\n</div>\n</div>\n</div>\n\n\n<div id=\"outline-container-sec-4\" class=\"outline-2\">\n<h2 id=\"sec-4\"><span class=\"section-number-2\">4</span> 参考文献</h2>\n<div class=\"outline-text-2\" id=\"text-4\">\n<ol class=\"org-ol\">\n<li>Zaharia, Matei, et al. \"Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing.\" Proceedings of the 9th USENIX conference on Networked Systems Design and Implementation. USENIX Association, 2012\n</li>\n<li>Zaharia, Matei, et al. \"Spark: cluster computing with working sets.\" Proceedings of the 2nd USENIX conference on Hot topics in cloud computing. Vol. 10. 2010\n</li>\n<li>Shalev-Shwartz, Shai, et al. \"Pegasos: Primal estimated sub-gradient solver for svm.\" Mathematical programming 127.1 (2011): 3-30\n</li>\n</ol>\n</div>\n</div>\n\nLast Updated 2017-08-09 三 16:31.<br>Render by <a href=\"https://github.com/CodeFalling/hexo-renderer-org\">hexo-renderer-org</a> with <a href=\"http://www.gnu.org/software/emacs/\">Emacs</a> 24.5.1 (<a href=\"http://orgmode.org\">Org</a> mode 8.2.10)\n"}
