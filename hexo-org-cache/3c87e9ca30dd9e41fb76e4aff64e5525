{"md5":"87f5be5bb18ffe47ff9aa952585c7ee5","content":"\n\n\n\n<div id=\"outline-container-sec-1\" class=\"outline-2\">\n<h2 id=\"sec-1\"><span class=\"section-number-2\">1</span> 安装</h2>\n<div class=\"outline-text-2\" id=\"text-1\">\n<ul class=\"org-ul\">\n<li>编译器: <code>g++&gt;4.8</code> 或者 <code>clang</code>\n</li>\n<li>依赖：BLAS 库比如 <code>libblas</code>, <code>openblas</code>\n  对于不同的场景，我们会需要依赖不同的库。在这里，我们暂时不使用 GPU,所以不安装 CUDA。\n</li>\n</ul>\n\n<p>\n<code>git clone --recursive https://github.com/dmlc/mxnet</code>\n</p>\n</div>\n<div id=\"outline-container-sec-1-1\" class=\"outline-3\">\n<h3 id=\"sec-1-1\"><span class=\"section-number-3\">1.1</span> 目录结构</h3>\n<div class=\"outline-text-3\" id=\"text-1-1\">\n<div class=\"org-src-container\">\n\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">|- mxnet</span><br><span class=\"line\">  |- make</span><br><span class=\"line\">  |- Makefile</span><br><span class=\"line\">  |- cmake</span><br><span class=\"line\">  |- CMakeLists.txt</span><br><span class=\"line\">  |- docker</span><br><span class=\"line\">  |- dmlc-core</span><br><span class=\"line\">  |- ps-lite</span><br><span class=\"line\">  |- mshadow</span><br><span class=\"line\">  |- include</span><br><span class=\"line\">  |- src</span><br><span class=\"line\">  |- scala-package</span><br><span class=\"line\">  |- R-package</span><br><span class=\"line\">  |- python</span><br><span class=\"line\">  |- ...</span><br></pre></td></tr></table></figure>\n</div>\n\n<p>\nmxnet 依赖于 dmlc-core，ps-lite 和 mshadow 三个项目。在我看来，mxnet 实际上可以分为两部分。一部分我称之为 mxnet core，另一部分我称之为 mxnet api。在 core 中，\ninclude 文件夹中定义了一套 c api 供其他语言比如 python，scala 调用。mxnet core 并没有实现完整的神经网络训练逻辑，它定义了神经网络如何做前向后向传播，但实际训练时的迭代次数,\nKV Store 的起停等逻辑则是包含在 mxnet api 中的，所以 python，scala 等接口都有一套自己的实现逻辑。\n</p>\n</div>\n</div>\n\n<div id=\"outline-container-sec-1-2\" class=\"outline-3\">\n<h3 id=\"sec-1-2\"><span class=\"section-number-3\">1.2</span> 编译</h3>\n<div class=\"outline-text-3\" id=\"text-1-2\">\n<p>\nmxnet 现在有两套编译系统，一套直接基于 make，另一套基于 cmake。推荐使用 make，因为功能更全。现在的 mxnet 的 cmake 脚本不支持编译 scala。\n</p>\n\n<p>\n可以通过编辑 <b>make/config.mk</b> 文件来配置编译选项。对于我们而言，我们暂时不使用 GPU。同时我们需要与 Spark 结合，所以需要分布式的 KV Store。\n在 <b>make/config.ml</b> 下，修改配置如下：\n</p>\n\n<div class=\"org-src-container\">\n\n<figure class=\"highlight\"><table><tr><td class=\"code\"><pre><span class=\"line\">USE_DISK_KVSTORE = 1</span><br></pre></td></tr></table></figure>\n</div>\n\n<p>\n因为分布式 KV Store 依赖于 protobuf 和 zmq，我们需要安装对应的依赖库。\n</p>\n\n<p>\n开始编译\n</p>\n\n<div class=\"org-src-container\">\n\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">cd</span> mxnet</span><br><span class=\"line\">make</span><br><span class=\"line\">make scalapkg   <span class=\"comment\"># 如果你需要 scala 包</span></span><br><span class=\"line\">make scalatest  <span class=\"comment\"># 运行 scala 的测试用例</span></span><br></pre></td></tr></table></figure>\n</div>\n\n<p>\n若编译成功，你可以在 lib 目录下找到 libmxnet.so 文件。\n</p>\n</div>\n</div>\n</div>\n\n<div id=\"outline-container-sec-2\" class=\"outline-2\">\n<h2 id=\"sec-2\"><span class=\"section-number-2\">2</span> 参数服务器的优势</h2>\n<div class=\"outline-text-2\" id=\"text-2\">\n<p>\n现在 Spark 基本是大数据处理的事实标准，Spark MLlib 也实现了许多机器学习算法，但 Spark 其实仍是基于 Map/Reduce 计算模型的，而这一模型与机器学习算法的需求\n并不十分契合。在机器学习中，一个十分重要的步骤是计算参数的最优解，一般使用梯度下降方法：\n\\[\nw = w - \\lambda\\Delta w\n\\]\n</p>\n\n<p>\n在 Spark 中，每次迭代时，我们每个 partition 可以计算梯度，然后在 driver 端更新 weights。那么 driver 端必须等待所有 executor 完成梯度计算。一旦某个 executor 出现网络延时等问题，\n整个计算过程将受到影响。而参数服务器的目的既是消除这一影响，单个节点计算的延迟并不会影响整体的计算。使同步执行过程变成异步执行过程。比较 mxnet 和 sparkMLlib 中多层神经网络的训练时间，我们可以看到性能的差距。\n</p>\n\n\n<div class=\"figure\">\n<p><img src=\"mxnet/perf.png\" alt=\"perf.png\">\n</p>\n</div>\n</div>\n\n<div id=\"outline-container-sec-2-1\" class=\"outline-3\">\n<h3 id=\"sec-2-1\"><span class=\"section-number-3\">2.1</span> 实现方式</h3>\n<div class=\"outline-text-3\" id=\"text-2-1\">\n<p>\n在参数服务器中有三种角色：\n</p>\n<ol class=\"org-ol\">\n<li>worker： 计算梯度\n</li>\n<li>server： 从 worker 获取梯度信息，更新参数\n</li>\n<li>scheduler: 负责调度，worker 和 server 需 scheduler 注册信息\n</li>\n</ol>\n\n\n<div class=\"figure\">\n<p><img src=\"mxnet/arch.png\" alt=\"arch.png\">\n</p>\n</div>\n\n<p>\n工作流程：\n</p>\n<ol class=\"org-ol\">\n<li>worker，server 向 scheduler 注册，获得相关信息\n</li>\n<li>worker 从 server 端 pull 参数 w\n</li>\n<li>worker 基于参数 w 和数据计算梯度，然后 push 梯度到 server\n</li>\n<li>server 更新参数 w\n</li>\n<li>反复执行 2-4 这一过程\n</li>\n</ol>\n</div>\n</div>\n</div>\n\n<div id=\"outline-container-sec-3\" class=\"outline-2\">\n<h2 id=\"sec-3\"><span class=\"section-number-2\">3</span> 计算模型</h2>\n<div class=\"outline-text-2\" id=\"text-3\">\n<p>\n主要参考 mxnet 的两篇文章：\n</p>\n\n<p>\n<a href=\"http://mxnet.readthedocs.io/en/latest/system/program_model.html\">http://mxnet.readthedocs.io/en/latest/system/program_model.html</a>\n</p>\n\n<p>\n<a href=\"http://mxnet.readthedocs.io/en/latest/system/note_memory.html\">http://mxnet.readthedocs.io/en/latest/system/note_memory.html</a>\n</p>\n\n<p>\n对于用户而言，mxnet 提够了一套接口来定义神经网络。\n</p>\n\n<div class=\"org-src-container\">\n\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> data = <span class=\"type\">Symbol</span>.<span class=\"type\">Variable</span>(<span class=\"string\">\"data\"</span>)</span><br><span class=\"line\"><span class=\"keyword\">val</span> fc1 = <span class=\"type\">Symbol</span>.<span class=\"type\">FullyConnected</span>(name = <span class=\"string\">\"fc1\"</span>)(<span class=\"type\">Map</span>(<span class=\"string\">\"data\"</span> -&amp;gt; data, <span class=\"string\">\"num_hidden\"</span> -&amp;gt; <span class=\"number\">128</span>))</span><br><span class=\"line\"><span class=\"keyword\">val</span> act1 = <span class=\"type\">Symbol</span>.<span class=\"type\">Activation</span>(name = <span class=\"string\">\"relu1\"</span>)(<span class=\"type\">Map</span>(<span class=\"string\">\"data\"</span> -&amp;gt; fc1, <span class=\"string\">\"act_type\"</span> -&amp;gt; <span class=\"string\">\"relu\"</span>))</span><br><span class=\"line\"><span class=\"keyword\">val</span> fc2 = <span class=\"type\">Symbol</span>.<span class=\"type\">FullyConnected</span>(name = <span class=\"string\">\"fc2\"</span>)(<span class=\"type\">Map</span>(<span class=\"string\">\"data\"</span> -&amp;gt; act1, <span class=\"string\">\"num_hidden\"</span> -&amp;gt; <span class=\"number\">64</span>))</span><br><span class=\"line\"><span class=\"keyword\">val</span> act2 = <span class=\"type\">Symbol</span>.<span class=\"type\">Activation</span>(name = <span class=\"string\">\"relu2\"</span>)(<span class=\"type\">Map</span>(<span class=\"string\">\"data\"</span> -&amp;gt; fc2, <span class=\"string\">\"act_type\"</span> -&amp;gt; <span class=\"string\">\"relu\"</span>))</span><br><span class=\"line\"><span class=\"keyword\">val</span> fc3 = <span class=\"type\">Symbol</span>.<span class=\"type\">FullyConnected</span>(name = <span class=\"string\">\"fc3\"</span>)(<span class=\"type\">Map</span>(<span class=\"string\">\"data\"</span> -&amp;gt; act2, <span class=\"string\">\"num_hidden\"</span> -&amp;gt; <span class=\"number\">10</span>))</span><br><span class=\"line\"><span class=\"keyword\">val</span> mlp = <span class=\"type\">Symbol</span>.<span class=\"type\">SoftmaxOutput</span>(name = <span class=\"string\">\"softmax\"</span>)(<span class=\"type\">Map</span>(<span class=\"string\">\"data\"</span> -&amp;gt; fc3))</span><br></pre></td></tr></table></figure>\n</div>\n\n<p>\n如上一段 Scala 代码便定义了一个多层神经网络。而在实际执行时， <code>Symbol</code> 会调用 <code>toStaticGraph</code> 方法转成 <code>StaticGraph</code> 。\n<code>StaticGraph</code> 会计算图中节点的依赖并生成拓扑结构。我们知道训练神经网络有两个步骤，前向传播和后向传播。现在有两种不同的后向传播计算方法，\n一种是与前向传播共用一个图，而另一种则是显式生成后向传播图节点。\n</p>\n\n\n<div class=\"figure\">\n<p><img src=\"mxnet/back_graph.png\" alt=\"back_graph.png\">\n</p>\n</div>\n\n<p>\n有些深度学习库选择共用一个图，比如 caffe，torch。而另一些则选择显式后向传播节点，比如 Theano。mxnet 同样选择显式后向传播。这样可以为优化提供方便。\n</p>\n</div>\n</div>\n\n<div id=\"outline-container-sec-4\" class=\"outline-2\">\n<h2 id=\"sec-4\"><span class=\"section-number-2\">4</span> 实例</h2>\n<div class=\"outline-text-2\" id=\"text-4\">\n<p>\n我们先以一个实例来看看 mxnet 是如何运行的。鉴于 Spark 基本是当前大数据处理的事实标准，我们直接尝试将 mxnet 与 Spark 结合，\n从而更接近生产环境的工作流。mxnet 源码中已经有一个与 Spark 结合的实例，我们直接拿来分析。\n</p>\n\n<div class=\"org-src-container\">\n\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ClassificationExample</span></span><br><span class=\"line\"></span><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">ClassificationExample</span> {</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span>(</span>args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]): <span class=\"type\">Unit</span> = {</span><br><span class=\"line\">    <span class=\"keyword\">try</span> {</span><br><span class=\"line\">      <span class=\"comment\">// 初始化 SparkContext</span></span><br><span class=\"line\">      <span class=\"keyword\">val</span> conf = <span class=\"keyword\">new</span> <span class=\"type\">SparkConf</span>().setAppName(<span class=\"string\">\"MXNet\"</span>)</span><br><span class=\"line\">      <span class=\"keyword\">val</span> sc = <span class=\"keyword\">new</span> <span class=\"type\">SparkContext</span>(conf)</span><br><span class=\"line\"></span><br><span class=\"line\">      <span class=\"comment\">// 构建网络</span></span><br><span class=\"line\">      <span class=\"keyword\">val</span> network = <span class=\"keyword\">if</span> (cmdLine.model == <span class=\"string\">\"mlp\"</span>) getMlp <span class=\"keyword\">else</span> getLenet</span><br><span class=\"line\">      <span class=\"keyword\">val</span> dimension = <span class=\"keyword\">if</span> (cmdLine.model == <span class=\"string\">\"mlp\"</span>) <span class=\"type\">Shape</span>(<span class=\"number\">784</span>) <span class=\"keyword\">else</span> <span class=\"type\">Shape</span>(<span class=\"number\">1</span>, <span class=\"number\">28</span>, <span class=\"number\">28</span>)</span><br><span class=\"line\">      <span class=\"keyword\">val</span> devs =</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> (cmdLine.gpus != <span class=\"literal\">null</span>) cmdLine.gpus.split(',').map(id =&amp;gt; <span class=\"type\">Context</span>.gpu(id.trim.toInt))</span><br><span class=\"line\">\t<span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (cmdLine.cpus != <span class=\"literal\">null</span>) cmdLine.cpus.split(',').map(id =&amp;gt; <span class=\"type\">Context</span>.cpu(id.trim.toInt))</span><br><span class=\"line\">\t<span class=\"keyword\">else</span> <span class=\"type\">Array</span>(<span class=\"type\">Context</span>.cpu(<span class=\"number\">0</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">      <span class=\"comment\">// 配置训练属性</span></span><br><span class=\"line\">      <span class=\"keyword\">val</span> mxnet = <span class=\"keyword\">new</span> <span class=\"type\">MXNet</span>()</span><br><span class=\"line\">\t.setBatchSize(<span class=\"number\">128</span>)</span><br><span class=\"line\">\t.setLabelName(<span class=\"string\">\"softmax_label\"</span>)</span><br><span class=\"line\">\t.setContext(devs)</span><br><span class=\"line\">\t.setDimension(dimension)</span><br><span class=\"line\">\t.setNetwork(network)</span><br><span class=\"line\">\t.setNumEpoch(cmdLine.numEpoch)</span><br><span class=\"line\">\t.setNumServer(cmdLine.numServer)</span><br><span class=\"line\">\t.setNumWorker(cmdLine.numWorker)</span><br><span class=\"line\">\t.setExecutorJars(cmdLine.jars)</span><br><span class=\"line\">\t.setJava(cmdLine.java)</span><br><span class=\"line\"></span><br><span class=\"line\">      <span class=\"keyword\">val</span> trainData = parseRawData(sc, cmdLine.input)</span><br><span class=\"line\">      <span class=\"keyword\">val</span> start = <span class=\"type\">System</span>.currentTimeMillis</span><br><span class=\"line\"></span><br><span class=\"line\">      <span class=\"comment\">// 开始训练</span></span><br><span class=\"line\">      <span class=\"keyword\">val</span> model = mxnet.fit(trainData)</span><br><span class=\"line\">      <span class=\"keyword\">val</span> timeCost = <span class=\"type\">System</span>.currentTimeMillis - start</span><br><span class=\"line\">      logger.info(<span class=\"string\">\"Training cost {} milli seconds\"</span>, timeCost)</span><br><span class=\"line\">      model.save(sc, cmdLine.output + <span class=\"string\">\"/model\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">      logger.info(<span class=\"string\">\"Now do validation\"</span>)</span><br><span class=\"line\">      <span class=\"keyword\">val</span> valData = parseRawData(sc, cmdLine.inputVal)</span><br><span class=\"line\"></span><br><span class=\"line\">      <span class=\"comment\">// 广播模型用于预测</span></span><br><span class=\"line\">      <span class=\"keyword\">val</span> brModel = sc.broadcast(model)</span><br><span class=\"line\">      <span class=\"keyword\">val</span> res = valData.mapPartitions { data =&amp;gt;</span><br><span class=\"line\">\t<span class=\"comment\">// get real labels</span></span><br><span class=\"line\">\t<span class=\"keyword\">import</span> org.apache.spark.mllib.linalg.<span class=\"type\">Vector</span></span><br><span class=\"line\">\t<span class=\"keyword\">val</span> points = <span class=\"type\">ArrayBuffer</span>.empty[<span class=\"type\">Vector</span>]</span><br><span class=\"line\">\t<span class=\"keyword\">val</span> y = <span class=\"type\">ArrayBuffer</span>.empty[<span class=\"type\">Float</span>]</span><br><span class=\"line\">\t<span class=\"keyword\">while</span> (data.hasNext) {</span><br><span class=\"line\">\t  <span class=\"keyword\">val</span> evalData = data.next()</span><br><span class=\"line\">\t  y += evalData.label.toFloat</span><br><span class=\"line\">\t  points += evalData.features</span><br><span class=\"line\">\t}</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">// get predicted labels</span></span><br><span class=\"line\">\t<span class=\"keyword\">val</span> probArrays = brModel.value.predict(points.toIterator)</span><br><span class=\"line\">\trequire(probArrays.length == <span class=\"number\">1</span>)</span><br><span class=\"line\">\t<span class=\"keyword\">val</span> prob = probArrays(<span class=\"number\">0</span>)</span><br><span class=\"line\">\t<span class=\"keyword\">val</span> py = <span class=\"type\">NDArray</span>.argmaxChannel(prob.get)</span><br><span class=\"line\">\trequire(y.length == py.size, s<span class=\"string\">\"${y.length} mismatch ${py.size}\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">// I'm too lazy to calculate the accuracy</span></span><br><span class=\"line\">\t<span class=\"keyword\">val</span> res = <span class=\"type\">Iterator</span>((y.toArray zip py.toArray).map {</span><br><span class=\"line\">\t  <span class=\"keyword\">case</span> (y1, py1) =&amp;gt; y1 + <span class=\"string\">\",\"</span> + py1 }.mkString(<span class=\"string\">\"\\n\"</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">\tpy.dispose()</span><br><span class=\"line\">\tprob.get.dispose()</span><br><span class=\"line\">\tres</span><br><span class=\"line\">      }</span><br><span class=\"line\">      res.saveAsTextFile(cmdLine.output + <span class=\"string\">\"/data\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">      sc.stop()</span><br><span class=\"line\">    } <span class=\"keyword\">catch</span> {</span><br><span class=\"line\">      <span class=\"keyword\">case</span> e: <span class=\"type\">Throwable</span> =&amp;gt;</span><br><span class=\"line\">\tlogger.error(e.getMessage, e)</span><br><span class=\"line\">\tsys.exit(-<span class=\"number\">1</span>)</span><br><span class=\"line\">    }</span><br><span class=\"line\">  }</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getMlp</span>:</span> <span class=\"type\">Symbol</span> = {</span><br><span class=\"line\">    <span class=\"keyword\">val</span> data = <span class=\"type\">Symbol</span>.<span class=\"type\">Variable</span>(<span class=\"string\">\"data\"</span>)</span><br><span class=\"line\">    <span class=\"keyword\">val</span> fc1 = <span class=\"type\">Symbol</span>.<span class=\"type\">FullyConnected</span>(name = <span class=\"string\">\"fc1\"</span>)(<span class=\"type\">Map</span>(<span class=\"string\">\"data\"</span> -&amp;gt; data, <span class=\"string\">\"num_hidden\"</span> -&amp;gt; <span class=\"number\">128</span>))</span><br><span class=\"line\">    <span class=\"keyword\">val</span> act1 = <span class=\"type\">Symbol</span>.<span class=\"type\">Activation</span>(name = <span class=\"string\">\"relu1\"</span>)(<span class=\"type\">Map</span>(<span class=\"string\">\"data\"</span> -&amp;gt; fc1, <span class=\"string\">\"act_type\"</span> -&amp;gt; <span class=\"string\">\"relu\"</span>))</span><br><span class=\"line\">    <span class=\"keyword\">val</span> fc2 = <span class=\"type\">Symbol</span>.<span class=\"type\">FullyConnected</span>(name = <span class=\"string\">\"fc2\"</span>)(<span class=\"type\">Map</span>(<span class=\"string\">\"data\"</span> -&amp;gt; act1, <span class=\"string\">\"num_hidden\"</span> -&amp;gt; <span class=\"number\">64</span>))</span><br><span class=\"line\">    <span class=\"keyword\">val</span> act2 = <span class=\"type\">Symbol</span>.<span class=\"type\">Activation</span>(name = <span class=\"string\">\"relu2\"</span>)(<span class=\"type\">Map</span>(<span class=\"string\">\"data\"</span> -&amp;gt; fc2, <span class=\"string\">\"act_type\"</span> -&amp;gt; <span class=\"string\">\"relu\"</span>))</span><br><span class=\"line\">    <span class=\"keyword\">val</span> fc3 = <span class=\"type\">Symbol</span>.<span class=\"type\">FullyConnected</span>(name = <span class=\"string\">\"fc3\"</span>)(<span class=\"type\">Map</span>(<span class=\"string\">\"data\"</span> -&amp;gt; act2, <span class=\"string\">\"num_hidden\"</span> -&amp;gt; <span class=\"number\">10</span>))</span><br><span class=\"line\">    <span class=\"keyword\">val</span> mlp = <span class=\"type\">Symbol</span>.<span class=\"type\">SoftmaxOutput</span>(name = <span class=\"string\">\"softmax\"</span>)(<span class=\"type\">Map</span>(<span class=\"string\">\"data\"</span> -&amp;gt; fc3))</span><br><span class=\"line\">    mlp</span><br><span class=\"line\">  }</span><br><span class=\"line\">}</span><br></pre></td></tr></table></figure>\n</div>\n\n<p>\n为了与 Spark 沟通，毫无疑问首先是初始化 <code>SparkContext</code> 。然后我们需要定义神经网络， <code>getMlp</code> 方法通过 <code>Symbol</code> 定义了一个多层神经网络。然后新建 <code>MXNet</code> 类，定义训练属性。\n可以看到，接下来最关键的一步是 <code>mxnet.fit(trainData)</code> 。此方法接受一个 RDD,并获得最终模型。\n</p>\n\n<p>\n在 <code>mxnet.fit</code> 方法中，主要有以下几步操作：\n</p>\n<ol class=\"org-ol\">\n<li>新建一个 ParameterServer scheduler。这里存在一个问题，一旦 scheduler 挂了，整个参数服务器将不能运作，需要 HA 改进\n</li>\n<li>通过 Spark 每个 partition 新建一个 ParameterServer Server\n</li>\n<li>对于数据集，每个 partition 新建一个 ParameterServer worker\n</li>\n<li>每个 partition 新建一个  <code>FeedForword</code> 网络，对应每个 worker，调用 <code>FeedForword.fit</code> 进行训练。\n</li>\n</ol>\n\n\n<div class=\"org-src-container\">\n\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">fit</span>(</span>data: <span class=\"type\">RDD</span>[<span class=\"type\">LabeledPoint</span>]): <span class=\"type\">MXNetModel</span> = {</span><br><span class=\"line\">  <span class=\"keyword\">val</span> sc = data.context</span><br><span class=\"line\">  <span class=\"comment\">// distribute native jars</span></span><br><span class=\"line\">  params.jars.foreach(jar =&amp;gt; sc.addFile(jar))</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">val</span> trainData = {</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (params.numWorker &amp;gt; data.partitions.length) {</span><br><span class=\"line\">      logger.info(<span class=\"string\">\"repartitioning training set to {} partitions\"</span>, params.numWorker)</span><br><span class=\"line\">      data.repartition(params.numWorker)</span><br><span class=\"line\">    } <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (params.numWorker &amp;lt; data.partitions.length) {</span><br><span class=\"line\">      logger.info(<span class=\"string\">\"repartitioning training set to {} partitions\"</span>, params.numWorker)</span><br><span class=\"line\">      data.coalesce(params.numWorker)</span><br><span class=\"line\">    } <span class=\"keyword\">else</span> {</span><br><span class=\"line\">      data</span><br><span class=\"line\">    }</span><br><span class=\"line\">  }</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">val</span> schedulerIP = utils.<span class=\"type\">Network</span>.ipAddress</span><br><span class=\"line\">  <span class=\"keyword\">val</span> schedulerPort = utils.<span class=\"type\">Network</span>.availablePort</span><br><span class=\"line\">  <span class=\"comment\">// <span class=\"doctag\">TODO:</span> check ip &amp;amp; port available</span></span><br><span class=\"line\">  logger.info(<span class=\"string\">\"Starting scheduler on {}:{}\"</span>, schedulerIP, schedulerPort)</span><br><span class=\"line\">  <span class=\"keyword\">val</span> scheduler = <span class=\"keyword\">new</span> <span class=\"type\">ParameterServer</span>(params.runtimeClasspath, role = <span class=\"string\">\"scheduler\"</span>,</span><br><span class=\"line\">    rootUri = schedulerIP, rootPort = schedulerPort,</span><br><span class=\"line\">    numServer = params.numServer, numWorker = params.numWorker, java = params.javabin)</span><br><span class=\"line\">  require(scheduler.startProcess(), <span class=\"string\">\"Failed to start ps scheduler process\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">  sc.parallelize(<span class=\"number\">1</span> to params.numServer, params.numServer).foreachPartition { p =&amp;gt;</span><br><span class=\"line\">    logger.info(<span class=\"string\">\"Starting server ...\"</span>)</span><br><span class=\"line\">    <span class=\"keyword\">val</span> server = <span class=\"keyword\">new</span> <span class=\"type\">ParameterServer</span>(params.runtimeClasspath,</span><br><span class=\"line\">      role = <span class=\"string\">\"server\"</span>,</span><br><span class=\"line\">      rootUri = schedulerIP, rootPort = schedulerPort,</span><br><span class=\"line\">      numServer = params.numServer,</span><br><span class=\"line\">      numWorker = params.numWorker,</span><br><span class=\"line\">      java = params.javabin)</span><br><span class=\"line\">    require(server.startProcess(), <span class=\"string\">\"Failed to start ps server process\"</span>)</span><br><span class=\"line\">  }</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">val</span> job = trainData.mapPartitions { partition =&amp;gt;</span><br><span class=\"line\">    <span class=\"keyword\">val</span> dataIter = <span class=\"keyword\">new</span> <span class=\"type\">LabeledPointIter</span>(</span><br><span class=\"line\">      partition, params.dimension,</span><br><span class=\"line\">      params.batchSize,</span><br><span class=\"line\">      dataName = params.dataName,</span><br><span class=\"line\">      labelName = params.labelName)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// <span class=\"doctag\">TODO:</span> more nature way to get the # of examples?</span></span><br><span class=\"line\">    <span class=\"keyword\">var</span> numExamples = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">while</span> (dataIter.hasNext) {</span><br><span class=\"line\">      <span class=\"keyword\">val</span> dataBatch = dataIter.next()</span><br><span class=\"line\">      numExamples += dataBatch.label.head.shape(<span class=\"number\">0</span>)</span><br><span class=\"line\">    }</span><br><span class=\"line\">    logger.debug(<span class=\"string\">\"Number of samples: {}\"</span>, numExamples)</span><br><span class=\"line\">    dataIter.reset()</span><br><span class=\"line\"></span><br><span class=\"line\">    logger.info(<span class=\"string\">\"Launching worker ...\"</span>)</span><br><span class=\"line\">    logger.info(<span class=\"string\">\"Batch {}\"</span>, params.batchSize)</span><br><span class=\"line\">    <span class=\"type\">KVStoreServer</span>.init(<span class=\"type\">ParameterServer</span>.buildEnv(role = <span class=\"string\">\"worker\"</span>,</span><br><span class=\"line\">      rootUri = schedulerIP, rootPort = schedulerPort,</span><br><span class=\"line\">      numServer = params.numServer,</span><br><span class=\"line\">      numWorker = params.numWorker))</span><br><span class=\"line\">    <span class=\"keyword\">val</span> kv = <span class=\"type\">KVStore</span>.create(<span class=\"string\">\"dist_async\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">val</span> optimizer: <span class=\"type\">Optimizer</span> = <span class=\"keyword\">new</span> <span class=\"type\">SGD</span>(learningRate = <span class=\"number\">0.01</span>f,</span><br><span class=\"line\">      momentum = <span class=\"number\">0.9</span>f, wd = <span class=\"number\">0.00001</span>f)</span><br><span class=\"line\"></span><br><span class=\"line\">    logger.debug(<span class=\"string\">\"Define model\"</span>)</span><br><span class=\"line\">    <span class=\"keyword\">val</span> model = <span class=\"keyword\">new</span> <span class=\"type\">FeedForward</span>(ctx = params.context,</span><br><span class=\"line\">      symbol = params.getNetwork,</span><br><span class=\"line\">      numEpoch = params.numEpoch,</span><br><span class=\"line\">      optimizer = optimizer,</span><br><span class=\"line\">      initializer = <span class=\"keyword\">new</span> <span class=\"type\">Xavier</span>(factorType = <span class=\"string\">\"in\"</span>, magnitude = <span class=\"number\">2.34</span>f),</span><br><span class=\"line\">      argParams = <span class=\"literal\">null</span>,</span><br><span class=\"line\">      auxParams = <span class=\"literal\">null</span>,</span><br><span class=\"line\">      beginEpoch = <span class=\"number\">0</span>,</span><br><span class=\"line\">      epochSize = numExamples / params.batchSize / kv.numWorkers)</span><br><span class=\"line\">    logger.info(<span class=\"string\">\"Start training ...\"</span>)</span><br><span class=\"line\">    model.fit(trainData = dataIter,</span><br><span class=\"line\">      evalData = <span class=\"literal\">null</span>,</span><br><span class=\"line\">      evalMetric = <span class=\"keyword\">new</span> <span class=\"type\">Accuracy</span>(),</span><br><span class=\"line\">      kvStore = kv)</span><br><span class=\"line\"></span><br><span class=\"line\">    logger.info(<span class=\"string\">\"Training finished, waiting for other workers ...\"</span>)</span><br><span class=\"line\">    dataIter.dispose()</span><br><span class=\"line\">    kv.barrier()</span><br><span class=\"line\">    kv.dispose()</span><br><span class=\"line\">    <span class=\"type\">Iterator</span>(<span class=\"keyword\">new</span> <span class=\"type\">MXNetModel</span>(</span><br><span class=\"line\">      model, params.dimension, params.batchSize,</span><br><span class=\"line\">      dataName = params.dataName, labelName = params.labelName))</span><br><span class=\"line\">  }.cache()</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">// force job to run</span></span><br><span class=\"line\">  job.foreachPartition(() =&amp;gt; _)</span><br><span class=\"line\">  <span class=\"comment\">// simply the first model</span></span><br><span class=\"line\">  <span class=\"keyword\">val</span> mxModel = job.first()</span><br><span class=\"line\"></span><br><span class=\"line\">  logger.info(<span class=\"string\">\"Waiting for scheduler ...\"</span>)</span><br><span class=\"line\">  scheduler.waitFor()</span><br><span class=\"line\">  mxModel</span><br><span class=\"line\">}</span><br></pre></td></tr></table></figure>\n</div>\n\n<div class=\"org-src-container\">\n\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// FeedForword.fit</span></span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">fit</span>(</span>trainData: <span class=\"type\">DataIter</span>, evalData: <span class=\"type\">DataIter</span>, evalMetric: <span class=\"type\">EvalMetric</span> = <span class=\"keyword\">new</span> <span class=\"type\">Accuracy</span>(),</span><br><span class=\"line\">\t\t  kvStore: <span class=\"type\">Option</span>[<span class=\"type\">KVStore</span>], updateOnKVStore: <span class=\"type\">Boolean</span>,</span><br><span class=\"line\">\t\t  epochEndCallback: <span class=\"type\">EpochEndCallback</span> = <span class=\"literal\">null</span>,</span><br><span class=\"line\">\t\t  batchEndCallback: <span class=\"type\">BatchEndCallback</span> = <span class=\"literal\">null</span>, logger: <span class=\"type\">Logger</span> = <span class=\"type\">FeedForward</span>.logger,</span><br><span class=\"line\">\t\t  workLoadList: <span class=\"type\">Seq</span>[<span class=\"type\">Float</span>] = <span class=\"literal\">null</span>): <span class=\"type\">Unit</span> = {</span><br><span class=\"line\">    require(evalMetric != <span class=\"literal\">null</span>, <span class=\"string\">\"evalMetric cannot be null\"</span>)</span><br><span class=\"line\">    <span class=\"keyword\">val</span> (argNames, paramNames, auxNames) =</span><br><span class=\"line\">      initParams(trainData.provideData ++ trainData.provideLabel)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// init optimizer</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> batchSizeMultiplier = kvStore.map { kv =&amp;gt;</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (kv.`<span class=\"class\"><span class=\"keyword\">type</span>` <span class=\"title\">==</span> \"<span class=\"title\">dist_sync</span>\") {</span></span><br><span class=\"line\">\tkv.numWorkers</span><br><span class=\"line\">      } <span class=\"keyword\">else</span> {</span><br><span class=\"line\">\t<span class=\"number\">1</span></span><br><span class=\"line\">      }</span><br><span class=\"line\">    }</span><br><span class=\"line\">    <span class=\"keyword\">val</span> batchSize = trainData.batchSize * batchSizeMultiplier.getOrElse(<span class=\"number\">1</span>)</span><br><span class=\"line\">    <span class=\"keyword\">this</span>.optimizer.setArgNames(argNames)</span><br><span class=\"line\">    <span class=\"keyword\">this</span>.optimizer.setRescaleGrad(<span class=\"number\">1</span>f / batchSize)</span><br><span class=\"line\"></span><br><span class=\"line\">    logger.debug(<span class=\"string\">\"Start training on multi-device\"</span>)</span><br><span class=\"line\">    <span class=\"type\">Model</span>.trainMultiDevice(</span><br><span class=\"line\">      symbol, ctx, argNames, paramNames, auxNames,</span><br><span class=\"line\">      _argParams, _auxParams,</span><br><span class=\"line\">      <span class=\"keyword\">this</span>.beginEpoch, <span class=\"keyword\">this</span>.numEpoch,</span><br><span class=\"line\">      <span class=\"keyword\">this</span>.epochSize, <span class=\"keyword\">this</span>.optimizer,</span><br><span class=\"line\">      kvStore, updateOnKVStore,</span><br><span class=\"line\">      trainData = trainData, evalData = <span class=\"type\">Option</span>(evalData),</span><br><span class=\"line\">      evalMetric = evalMetric,</span><br><span class=\"line\">      epochEndCallback = <span class=\"type\">Option</span>(epochEndCallback),</span><br><span class=\"line\">      batchEndCallback = <span class=\"type\">Option</span>(batchEndCallback),</span><br><span class=\"line\">      logger = logger, workLoadList = workLoadList,</span><br><span class=\"line\">      monitor = monitor)</span><br></pre></td></tr></table></figure>\n</div>\n\n<p>\n可以看到，在 <code>FeedForword.fit</code> 中，基本上是直接调用了 <code>Model.trainMultiDevice</code> 方法。而此方法则实现了神经网络的前向后向传播和 KV store 的更新。\n主要步骤：\n</p>\n<ol class=\"org-ol\">\n<li>取 batch\n</li>\n<li>在此 batch 上做 forward 和 backward 传播\n</li>\n<li>从 kv store 更新参数\n</li>\n</ol>\n\n<div class=\"org-src-container\">\n\n<figure class=\"highlight scala\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span>[mxnet] <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">trainMultiDevice</span>(</span>symbol: <span class=\"type\">Symbol</span>, ctx: <span class=\"type\">Array</span>[<span class=\"type\">Context</span>],</span><br><span class=\"line\">\t\t\t\t    argNames: <span class=\"type\">Seq</span>[<span class=\"type\">String</span>], paramNames: <span class=\"type\">Seq</span>[<span class=\"type\">String</span>],</span><br><span class=\"line\">\t\t\t\t    auxNames: <span class=\"type\">Seq</span>[<span class=\"type\">String</span>], argParams: <span class=\"type\">Map</span>[<span class=\"type\">String</span>, <span class=\"type\">NDArray</span>],</span><br><span class=\"line\">\t\t\t\t    auxParams: <span class=\"type\">Map</span>[<span class=\"type\">String</span>, <span class=\"type\">NDArray</span>],</span><br><span class=\"line\">\t\t\t\t    beginEpoch: <span class=\"type\">Int</span>, endEpoch: <span class=\"type\">Int</span>, epochSize: <span class=\"type\">Int</span>,</span><br><span class=\"line\">\t\t\t\t    optimizer: <span class=\"type\">Optimizer</span>,</span><br><span class=\"line\">\t\t\t\t    kvStore: <span class=\"type\">Option</span>[<span class=\"type\">KVStore</span>], updateOnKVStore: <span class=\"type\">Boolean</span>,</span><br><span class=\"line\">\t\t\t\t    trainData: <span class=\"type\">DataIter</span> = <span class=\"literal\">null</span>,</span><br><span class=\"line\">\t\t\t\t    evalData: <span class=\"type\">Option</span>[<span class=\"type\">DataIter</span>] = <span class=\"type\">None</span>,</span><br><span class=\"line\">\t\t\t\t    evalMetric: <span class=\"type\">EvalMetric</span>,</span><br><span class=\"line\">\t\t\t\t    epochEndCallback: <span class=\"type\">Option</span>[<span class=\"type\">EpochEndCallback</span>] = <span class=\"type\">None</span>,</span><br><span class=\"line\">\t\t\t\t    batchEndCallback: <span class=\"type\">Option</span>[<span class=\"type\">BatchEndCallback</span>] = <span class=\"type\">None</span>,</span><br><span class=\"line\">\t\t\t\t    logger: <span class=\"type\">Logger</span> = logger,</span><br><span class=\"line\">\t\t\t\t    workLoadList: <span class=\"type\">Seq</span>[<span class=\"type\">Float</span>] = <span class=\"type\">Nil</span>,</span><br><span class=\"line\">\t\t\t\t    monitor: <span class=\"type\">Option</span>[<span class=\"type\">Monitor</span>] = <span class=\"type\">None</span>): <span class=\"type\">Unit</span> = {</span><br><span class=\"line\">  <span class=\"keyword\">val</span> executorManager = <span class=\"keyword\">new</span> <span class=\"type\">DataParallelExecutorManager</span>(</span><br><span class=\"line\">      symbol = symbol,</span><br><span class=\"line\">      ctx = ctx,</span><br><span class=\"line\">      trainData = trainData,</span><br><span class=\"line\">      paramNames = paramNames,</span><br><span class=\"line\">      argNames = argNames,</span><br><span class=\"line\">      auxNames = auxNames,</span><br><span class=\"line\">      workLoadList = workLoadList,</span><br><span class=\"line\">      logger = logger)</span><br><span class=\"line\"></span><br><span class=\"line\">  monitor.foreach(executorManager.installMonitor)</span><br><span class=\"line\">  executorManager.setParams(argParams, auxParams)</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">// updater for updateOnKVStore = false</span></span><br><span class=\"line\">  <span class=\"keyword\">val</span> updaterLocal = <span class=\"type\">Optimizer</span>.getUpdater(optimizer)</span><br><span class=\"line\"></span><br><span class=\"line\">  kvStore.foreach(initializeKVStore(_, executorManager.paramArrays,</span><br><span class=\"line\">    argParams, executorManager._paramNames, updateOnKVStore))</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (updateOnKVStore) {</span><br><span class=\"line\">    kvStore.foreach(_.setOptimizer(optimizer))</span><br><span class=\"line\">  }</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">// Now start training</span></span><br><span class=\"line\">  <span class=\"keyword\">for</span> (epoch &amp;lt;- beginEpoch until endEpoch) {</span><br><span class=\"line\">    <span class=\"comment\">// Training phase</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> tic = <span class=\"type\">System</span>.currentTimeMillis</span><br><span class=\"line\">    evalMetric.reset()</span><br><span class=\"line\">    <span class=\"keyword\">var</span> nBatch = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">var</span> epochDone = <span class=\"literal\">false</span></span><br><span class=\"line\">    <span class=\"comment\">// Iterate over training data.</span></span><br><span class=\"line\">    trainData.reset()</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (!epochDone) {</span><br><span class=\"line\">      <span class=\"keyword\">var</span> doReset = <span class=\"literal\">true</span></span><br><span class=\"line\">      <span class=\"keyword\">while</span> (doReset &amp;amp;&amp;amp; trainData.hasNext) {</span><br><span class=\"line\">\t<span class=\"keyword\">val</span> dataBatch = trainData.next()</span><br><span class=\"line\">\texecutorManager.loadDataBatch(dataBatch)</span><br><span class=\"line\">\tmonitor.foreach(_.tic())</span><br><span class=\"line\">\texecutorManager.forward(isTrain = <span class=\"literal\">true</span>)</span><br><span class=\"line\">\texecutorManager.backward()</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> (updateOnKVStore) {</span><br><span class=\"line\">\t  updateParamsOnKVStore(executorManager.paramArrays,</span><br><span class=\"line\">\t    executorManager.gradArrays,</span><br><span class=\"line\">\t    kvStore)</span><br><span class=\"line\">\t} <span class=\"keyword\">else</span> {</span><br><span class=\"line\">\t  updateParams(executorManager.paramArrays,</span><br><span class=\"line\">\t    executorManager.gradArrays,</span><br><span class=\"line\">\t    updaterLocal, ctx.length,</span><br><span class=\"line\">\t    kvStore)</span><br><span class=\"line\">\t}</span><br><span class=\"line\">\tmonitor.foreach(_.tocPrint())</span><br><span class=\"line\">\t<span class=\"comment\">// evaluate at end, so out_cpu_array can lazy copy</span></span><br><span class=\"line\">\tevalMetric.update(dataBatch.label, executorManager.cpuOutputArrays)</span><br><span class=\"line\"></span><br><span class=\"line\">\tnBatch += <span class=\"number\">1</span></span><br><span class=\"line\">\tbatchEndCallback.foreach(_.invoke(epoch, nBatch, evalMetric))</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">// this epoch is done possibly earlier</span></span><br><span class=\"line\">\t<span class=\"keyword\">if</span> (epochSize != -<span class=\"number\">1</span> &amp;amp;&amp;amp; nBatch &amp;gt;= epochSize) {</span><br><span class=\"line\">\t  doReset = <span class=\"literal\">false</span></span><br><span class=\"line\">\t}</span><br><span class=\"line\">\tdataBatch.dispose()</span><br><span class=\"line\">      }</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (doReset) {</span><br><span class=\"line\">\ttrainData.reset()</span><br><span class=\"line\">      }</span><br><span class=\"line\"></span><br><span class=\"line\">      <span class=\"comment\">// this epoch is done</span></span><br><span class=\"line\">      epochDone = (epochSize == -<span class=\"number\">1</span> || nBatch &amp;gt;= epochSize)</span><br><span class=\"line\">    }</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">val</span> (name, value) = evalMetric.get</span><br><span class=\"line\">    logger.info(s<span class=\"string\">\"Epoch[$epoch] Train-$name=$value\"</span>)</span><br><span class=\"line\">    <span class=\"keyword\">val</span> toc = <span class=\"type\">System</span>.currentTimeMillis</span><br><span class=\"line\">    logger.info(s<span class=\"string\">\"Epoch[$epoch] Time cost=${toc - tic}\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    evalData.foreach { evalDataIter =&amp;gt;</span><br><span class=\"line\">      evalMetric.reset()</span><br><span class=\"line\">      evalDataIter.reset()</span><br><span class=\"line\">      <span class=\"comment\">// <span class=\"doctag\">TODO:</span> make DataIter implement Iterator</span></span><br><span class=\"line\">      <span class=\"keyword\">while</span> (evalDataIter.hasNext) {</span><br><span class=\"line\">\t<span class=\"keyword\">val</span> evalBatch = evalDataIter.next()</span><br><span class=\"line\">\texecutorManager.loadDataBatch(evalBatch)</span><br><span class=\"line\">\texecutorManager.forward(isTrain = <span class=\"literal\">false</span>)</span><br><span class=\"line\">\tevalMetric.update(evalBatch.label, executorManager.cpuOutputArrays)</span><br><span class=\"line\">\tevalBatch.dispose()</span><br><span class=\"line\">      }</span><br><span class=\"line\"></span><br><span class=\"line\">      <span class=\"keyword\">val</span> (name, value) = evalMetric.get</span><br><span class=\"line\">      logger.info(s<span class=\"string\">\"Epoch[$epoch] Validation-$name=$value\"</span>)</span><br><span class=\"line\">    }</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (epochEndCallback.isDefined || epoch + <span class=\"number\">1</span> == endEpoch) {</span><br><span class=\"line\">      executorManager.copyTo(argParams, auxParams)</span><br><span class=\"line\">    }</span><br><span class=\"line\">    epochEndCallback.foreach(_.invoke(epoch, symbol, argParams, auxParams))</span><br><span class=\"line\">  }</span><br><span class=\"line\"></span><br><span class=\"line\">  updaterLocal.dispose()</span><br><span class=\"line\">  executorManager.dispose()</span><br><span class=\"line\">}</span><br></pre></td></tr></table></figure>\n</div>\n</div>\n</div>\n\n<div id=\"outline-container-sec-5\" class=\"outline-2\">\n<h2 id=\"sec-5\"><span class=\"section-number-2\">5</span> 组件</h2>\n<div class=\"outline-text-2\" id=\"text-5\">\n</div><div id=\"outline-container-sec-5-1\" class=\"outline-3\">\n<h3 id=\"sec-5-1\"><span class=\"section-number-3\">5.1</span> dmlc-core</h3>\n<div class=\"outline-text-3\" id=\"text-5-1\">\n</div><div id=\"outline-container-sec-5-1-1\" class=\"outline-4\">\n<h4 id=\"sec-5-1-1\"><span class=\"section-number-4\">5.1.1</span> parameter.h</h4>\n<div class=\"outline-text-4\" id=\"text-5-1-1\">\n<p>\n与 spark 类似，dmlc core 也有一套定义参数的系统。cpp 没有类似 java 的反射机制，\n所以在 dmlc 中用到的方法比较 hack：计算类中属性的 offset。\n</p>\n</div>\n</div>\n<div id=\"outline-container-sec-5-1-2\" class=\"outline-4\">\n<h4 id=\"sec-5-1-2\"><span class=\"section-number-4\">5.1.2</span> data.h</h4>\n</div>\n</div>\n\n<div id=\"outline-container-sec-5-2\" class=\"outline-3\">\n<h3 id=\"sec-5-2\"><span class=\"section-number-3\">5.2</span> ps-lite</h3>\n<div class=\"outline-text-3\" id=\"text-5-2\">\n<p>\npostoffice\nserver, worker, scheduler\nControl: empty, terminate, add<sub>node</sub>, barrier, ack\nvan\nmessage\n新建 KVWorker 和 KVServer 包含 Customer，初始化时新建一个线程用于接收消息\n</p>\n\n<div class=\"org-src-container\">\n\n<figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\">Customer::Customer(<span class=\"keyword\">int</span> id, <span class=\"keyword\">const</span> Customer::RecvHandle&amp;amp; recv_handle)</span><br><span class=\"line\">    : id_(id), recv_handle_(recv_handle) {</span><br><span class=\"line\">  Postoffice::Get()-&amp;gt;AddCustomer(<span class=\"keyword\">this</span>);</span><br><span class=\"line\">  recv_thread_ = <span class=\"built_in\">std</span>::unique_ptr&amp;lt;<span class=\"built_in\">std</span>::thread&amp;gt;(<span class=\"keyword\">new</span> <span class=\"built_in\">std</span>::thread(&amp;amp;Customer::Receiving, <span class=\"keyword\">this</span>));</span><br><span class=\"line\">}</span><br></pre></td></tr></table></figure>\n</div>\n\n<p>\nvan 封装通信，现在使用 zmq\n</p>\n</div>\n</div>\n\n<div id=\"outline-container-sec-5-3\" class=\"outline-3\">\n<h3 id=\"sec-5-3\"><span class=\"section-number-3\">5.3</span> mxnet</h3>\n</div>\n</div>\n\nLast Updated 2017-08-09 三 16:31.<br>Render by <a href=\"https://github.com/CodeFalling/hexo-renderer-org\">hexo-renderer-org</a> with <a href=\"http://www.gnu.org/software/emacs/\">Emacs</a> 24.5.1 (<a href=\"http://orgmode.org\">Org</a> mode 8.2.10)\n"}
